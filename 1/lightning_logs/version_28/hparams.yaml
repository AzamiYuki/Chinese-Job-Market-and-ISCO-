adam_epsilon: 1.0e-08
chinese_model_optimization: true
eval_batch_size: 16
eval_top_k: 10
freeze_transformer: false
hidden_dropout: 0.0
labels_groups: null
labels_groups_mapping: null
labels_paths: null
language: zh
learning_rate: 2.0e-05
model_name_or_path: bert-base-multilingual-cased
output_size: 435
train_batch_size: 16
verbose: false
warmup_steps: 50
weight_decay: 0.01
