import pandas as pd
import numpy as np
import json
import networkx as nx
from collections import defaultdict
import random
from typing import Dict, List, Tuple
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score
import warnings
from datetime import datetime
warnings.filterwarnings('ignore')
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score


class PALinkPrediction:
    def __init__(self, csv_file: str, random_seed: int = 42):
        """
        PA (Preferential Attachment) é“¾æ¥é¢„æµ‹ç³»ç»Ÿ
        
        Args:
            csv_file: åŒ–ç®€åçš„æŠ€èƒ½æ•°æ®CSVæ–‡ä»¶
            random_seed: éšæœºç§å­
        """
        self.csv_file = csv_file
        self.random_seed = random_seed
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        # å›¾ç»“æ„
        self.full_graph = nx.Graph()
        self.training_graph = nx.Graph()
        
        # èŠ‚ç‚¹ä¿¡æ¯
        self.skill_nodes = set()
        self.job_nodes = set()
        
        # æ•°æ®é›†
        self.train_positive = []
        self.val_positive = []
        self.test_positive = []
        self.train_negative = []
        self.val_negative = []
        self.test_negative = []
        
        # PAç»“æœ
        self.pa_results = None
        
        print(f"PAé“¾æ¥é¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    

    def load_simplified_data_and_build_graph(self) -> Dict:
        """ä»åŒ–ç®€åçš„CSVåŠ è½½æ•°æ®å¹¶æ„å»ºå›¾"""
        print("æ­£åœ¨ä»åŒ–ç®€æ•°æ®æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        df = pd.read_csv(self.csv_file)
        print(f"è¯»å–æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # ç»Ÿè®¡è¾¹çš„æƒé‡
        edge_weights = defaultdict(int)
        
        for idx, row in df.iterrows():
            if idx % 5000 == 0:
                print(f"å¤„ç†è¿›åº¦: {idx}/{len(df)}")
            
            # è·å–ISCOèŒä¸šä»£ç 
            isco_code = row.get('ISCO_4_Digit_Code_Gemini')
            if pd.isna(isco_code):
                continue
            
            job_node = f"JOB_{int(isco_code)}"
            self.job_nodes.add(job_node)
            
            # è·å–åŒ–ç®€åçš„æŠ€èƒ½
            skills_str = row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]')
            try:
                skills = json.loads(skills_str)
            except:
                continue
            
            # ä¸ºæ¯ä¸ªæŠ€èƒ½åˆ›å»ºè¾¹
            for skill in skills:
                if skill and len(skill.strip()) > 1:
                    skill_node = f"SKILL_{skill.strip()}"
                    self.skill_nodes.add(skill_node)
                    
                    edge = (skill_node, job_node)
                    edge_weights[edge] += 1
        
        # æ„å»ºå›¾
        for (skill_node, job_node), weight in edge_weights.items():
            self.full_graph.add_edge(skill_node, job_node, weight=weight)
        
        # æ·»åŠ èŠ‚ç‚¹ç±»å‹
        for skill_node in self.skill_nodes:
            self.full_graph.add_node(skill_node, node_type='skill')
        for job_node in self.job_nodes:
            self.full_graph.add_node(job_node, node_type='job')
        
        stats = {
            'total_nodes': len(self.skill_nodes) + len(self.job_nodes),
            'skill_nodes': len(self.skill_nodes),
            'job_nodes': len(self.job_nodes),
            'total_edges': len(edge_weights),
            'density': nx.density(self.full_graph)
        }
        
        print(f"âœ“ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ:")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']} ({stats['skill_nodes']}æŠ€èƒ½ + {stats['job_nodes']}èŒä¸š)")
        print(f"  æ€»è¾¹æ•°: {stats['total_edges']}")
        print(f"  å›¾å¯†åº¦: {stats['density']:.6f}")
        
        return stats
    
    def split_and_prepare_datasets(self):
        """åˆ†å‰²æ•°æ®é›†å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("æ­£åœ¨åˆ†å‰²æ•°æ®é›†...")
        
        all_edges = list(self.full_graph.edges())
        total_edges = len(all_edges)
        
        # æŒ‰è®ºæ–‡æ¯”ä¾‹åˆ†å‰²
        train_size = int(total_edges * 0.55)
        val_size = int(total_edges * 0.15)
        
        # éšæœºæ‰“ä¹±
        random.shuffle(all_edges)
        
        self.train_positive = all_edges[:train_size]
        self.val_positive = all_edges[train_size:train_size + val_size]
        self.test_positive = all_edges[train_size + val_size:]
        
        # æ„å»ºè®­ç»ƒå›¾
        self.training_graph.clear()
        for node in self.full_graph.nodes(data=True):
            self.training_graph.add_node(node[0], **node[1])
        
        for edge in self.train_positive:
            weight = self.full_graph[edge[0]][edge[1]]['weight']
            self.training_graph.add_edge(edge[0], edge[1], weight=weight)
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬
        self._generate_negative_samples()
        
        print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_positive)} æ­£æ ·æœ¬ + {len(self.train_negative)} è´Ÿæ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(self.val_positive)} æ­£æ ·æœ¬ + {len(self.val_negative)} è´Ÿæ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(self.test_positive)} æ­£æ ·æœ¬ + {len(self.test_negative)} è´Ÿæ ·æœ¬")
    
    def _generate_negative_samples(self):
        """ç”Ÿæˆè´Ÿæ ·æœ¬"""
        existing_edges = set(self.full_graph.edges())
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æŠ€èƒ½-èŒä¸šé…å¯¹ä½œä¸ºè´Ÿæ ·æœ¬å€™é€‰
        negative_candidates = []
        for skill_node in self.skill_nodes:
            for job_node in self.job_nodes:
                if (skill_node, job_node) not in existing_edges and (job_node, skill_node) not in existing_edges:
                    negative_candidates.append((skill_node, job_node))
        
        random.shuffle(negative_candidates)
        
        # ä¸ºå„æ•°æ®é›†åˆ†é…è´Ÿæ ·æœ¬
        train_neg_size = len(self.train_positive)
        val_neg_size = len(self.val_positive)
        test_neg_size = len(self.test_positive)
        
        self.train_negative = negative_candidates[:train_neg_size]
        self.val_negative = negative_candidates[train_neg_size:train_neg_size + val_neg_size]
        self.test_negative = negative_candidates[train_neg_size + val_neg_size:train_neg_size + val_neg_size + test_neg_size]
    def find_optimal_threshold(self, scores: List[float], labels: List[int], metric: str = 'f1') -> Tuple[float, float]:
        """åœ¨ scores/labels ä¸Šæ‰«æé˜ˆå€¼ï¼Œè¿”å› (best_threshold, best_metric_value)ã€‚"""
        best_thr, best_score = 0.0, -1.0
        for thr in np.linspace(0.0, 1.0, 200):
            preds = (np.array(scores) >= thr).astype(int)
            # è·³è¿‡å…¨ 0/å…¨ 1 çš„æƒ…å†µ
            if len(set(preds)) < 2:
                continue
            if metric == 'f1':
                score = f1_score(labels, preds, zero_division=0)
            elif metric == 'precision':
                score = precision_score(labels, preds, zero_division=0)
            elif metric == 'recall':
                score = recall_score(labels, preds, zero_division=0)
            else:
                score = f1_score(labels, preds, zero_division=0)
            if score > best_score:
                best_score, best_thr = score, thr
        return best_thr, best_score
    def calculate_pa_scores(self) -> Dict:
        """è®¡ç®—å¹¶è¯„ä¼° PA ç®—æ³•ï¼Œå¸¦é˜ˆå€¼ä¼˜åŒ–"""
        # â€”â€” 1. è®¡ç®— raw PA åˆ†æ•° â€”â€” 
        node_degrees = dict(self.training_graph.degree())
        def pa_scores(edges):
            return [node_degrees.get(u,0)*node_degrees.get(v,0) for u,v in edges]

        pos_all = self.train_positive + self.val_positive + self.test_positive
        neg_all = self.train_negative + self.val_negative + self.test_negative
        pos_scores = pa_scores(pos_all)
        neg_scores = pa_scores(neg_all)

        # â€”â€” 2. å½’ä¸€åŒ– â€”â€” 
        all_scores = pos_scores + neg_scores
        max_s = max(all_scores) or 1
        pos_norm = [s/max_s for s in pos_scores]
        neg_norm = [s/max_s for s in neg_scores]

        # â€”â€” 3. åˆ‡åˆ†å›å„é›† â€”â€” 
        n_train, n_val, n_test = len(self.train_positive), len(self.val_positive), len(self.test_positive)
        train_scores = pos_norm[:n_train]     + neg_norm[:n_train]
        train_labels = [1]*n_train + [0]*n_train
        val_scores   = pos_norm[n_train:n_train+n_val] + neg_norm[n_train:n_train+n_val]
        val_labels   = [1]*n_val   + [0]*n_val
        test_scores  = pos_norm[n_train+n_val:]     + neg_norm[n_train+n_val:]
        test_labels  = [1]*n_test  + [0]*n_test

        # â€”â€” 4. éªŒè¯é›†ä¸Šæ‰¾æœ€ä¼˜é˜ˆå€¼ â€”â€” 
        best_thr, best_f1 = self.find_optimal_threshold(val_scores, val_labels, metric='f1')
        print(f"PA æœ€ä¼˜é˜ˆå€¼ (éªŒè¯é›†): {best_thr:.3f}, F1 = {best_f1:.4f}")

        # â€”â€” 5. ç”¨è¿™ä¸ªé˜ˆå€¼è¯„ä¼°æ‰€æœ‰æ•°æ®é›† â€”â€” 
        def eval_at_thr(scores, labels):
            preds = (np.array(scores) >= best_thr).astype(int)
            return {
                'precision': precision_score(labels, preds, zero_division=0),
                'recall':    recall_score(labels, preds, zero_division=0),
                'f1_score':  f1_score(labels, preds, zero_division=0),
                'auc_roc':   roc_auc_score(labels, scores),
                'auc_pr':    average_precision_score(labels, scores)
            }

        self.pa_results = {
            'train_metrics': eval_at_thr(train_scores, train_labels),
            'val_metrics':   eval_at_thr(val_scores,   val_labels),
            'test_metrics':  eval_at_thr(test_scores,  test_labels),
            'best_threshold': best_thr
        }

        # â€”â€” 6. æ‰“å°ç»“æœ â€”â€” 
        for split in ['train', 'val', 'test']:
            m = self.pa_results[f'{split}_metrics']
            print(f"\n{split.upper()} @ thr={best_thr:.3f}:"
                  f"  Precision={m['precision']:.4f}"
                  f"  Recall={m['recall']:.4f}"
                  f"  F1={m['f1_score']:.4f}"
                  f"  AUC-ROC={m['auc_roc']:.4f}"
                  f"  AUC-PR={m['auc_pr']:.4f}")

        return self.pa_results
    
    def save_results(self, output_file: str = 'pa_link_prediction_results.json'):
        """ä¿å­˜PAç®—æ³•ç»“æœ"""
        print(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ° {output_file}...")
        
        if not self.pa_results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        results_data = {
            'algorithm': 'Preferential Attachment (PA)',
            'graph_stats': {
                'total_nodes': len(self.skill_nodes) + len(self.job_nodes),
                'skill_nodes': len(self.skill_nodes),
                'job_nodes': len(self.job_nodes),
                'total_edges': self.full_graph.number_of_edges(),
                'density': nx.density(self.full_graph)
            },
            'dataset_splits': {
                'train_positive': len(self.train_positive),
                'train_negative': len(self.train_negative),
                'val_positive': len(self.val_positive),
                'val_negative': len(self.val_negative),
                'test_positive': len(self.test_positive),
                'test_negative': len(self.test_negative)
            },
            'performance_metrics': self.pa_results,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ° {output_file}")


def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡ŒPAç®—æ³•é“¾æ¥é¢„æµ‹"""
    print("=" * 80)
    print("PA (Preferential Attachment) é“¾æ¥é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 80)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    pa_predictor = PALinkPrediction(
        csv_file='simplified_jobs_skills.csv',  # ä½¿ç”¨åŒ–ç®€åçš„æ•°æ®
        random_seed=42
    )
    
    # æ­¥éª¤1: æ„å»ºå›¾
    print("\n" + "="*60)
    print("æ­¥éª¤1: æ„å»ºçŸ¥è¯†å›¾è°±")
    print("="*60)
    graph_stats = pa_predictor.load_simplified_data_and_build_graph()
    
    # æ­¥éª¤2: åˆ†å‰²æ•°æ®é›†
    print("\n" + "="*60)
    print("æ­¥éª¤2: åˆ†å‰²æ•°æ®é›†")
    print("="*60)
    pa_predictor.split_and_prepare_datasets()
    
    # æ­¥éª¤3: è®¡ç®—PAç®—æ³•æ€§èƒ½
    print("\n" + "="*60)
    print("æ­¥éª¤3: è®¡ç®—PAç®—æ³•é“¾æ¥é¢„æµ‹æ€§èƒ½")
    print("="*60)
    pa_results = pa_predictor.calculate_pa_scores()
    
    # æ­¥éª¤4: ä¿å­˜ç»“æœ
    print("\n" + "="*60)
    print("æ­¥éª¤4: ä¿å­˜ç»“æœ")
    print("="*60)
    pa_predictor.save_results()
    
    print("\n" + "="*80)
    print("ğŸ‰ PAé“¾æ¥é¢„æµ‹å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()