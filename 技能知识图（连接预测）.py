import pandas as pd
import numpy as np
import json
import networkx as nx
from collections import defaultdict, Counter
import random
from typing import Dict, List, Tuple, Set
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score
import warnings
warnings.filterwarnings('ignore')

class SkillJobKnowledgeGraph:
    def __init__(self, csv_file: str, random_seed: int = 42):
        """
        åˆå§‹åŒ–æŠ€èƒ½-èŒä¸šçŸ¥è¯†å›¾è°±æ„å»ºå™¨
        
        Args:
            csv_file: åŒ…å«æŠ€èƒ½æå–ç»“æœçš„CSVæ–‡ä»¶è·¯å¾„
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
        """
        self.csv_file = csv_file
        self.random_seed = random_seed
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        # å›¾ç»“æ„
        self.full_graph = nx.Graph()  # å®Œæ•´å›¾
        self.training_graph = nx.Graph()  # è®­ç»ƒå›¾
        
        # èŠ‚ç‚¹ä¿¡æ¯
        self.skill_nodes = set()
        self.job_nodes = set()
        
        # è¾¹ä¿¡æ¯
        self.all_positive_edges = []
        self.negative_candidate_pool = []
        
        # æ•°æ®é›†åˆ†å‰²
        self.train_positive = []
        self.val_positive = []
        self.test_positive = []
        self.train_negative = []
        self.val_negative = []
        self.test_negative = []
        
        # è¯„åˆ†ç»“æœ
        self.scores = {}
        
        print(f"çŸ¥è¯†å›¾è°±æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆï¼ˆéšæœºç§å­: {random_seed}ï¼‰")
    
    def load_data_and_build_graph(self) -> Dict:
        """
        ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®å¹¶æ„å»ºçŸ¥è¯†å›¾è°±
        
        Returns:
            Dict: å›¾çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        """
        print("æ­£åœ¨åŠ è½½æ•°æ®å¹¶æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(self.csv_file)
        print(f"è¯»å–æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # ç»Ÿè®¡è¾¹çš„æƒé‡ï¼ˆæŠ€èƒ½-èŒä¸šå…±ç°æ¬¡æ•°ï¼‰
        edge_weights = defaultdict(int)
        
        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            if idx % 5000 == 0:
                print(f"å¤„ç†è¿›åº¦: {idx}/{len(df)}")
            
            # è·å–ISCOèŒä¸šä»£ç 
            isco_code = row.get('ISCO_4_Digit_Code_Gemini')
            if pd.isna(isco_code):
                continue
            
            job_node = f"JOB_{int(isco_code)}"
            self.job_nodes.add(job_node)
            
            # è·å–æ ‡å‡†åŒ–æŠ€èƒ½
            normalized_skills_str = row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]')
            try:
                normalized_skills = json.loads(normalized_skills_str)
            except:
                continue
            
            # ä¸ºæ¯ä¸ªæŠ€èƒ½åˆ›å»ºè¾¹
            for skill in normalized_skills:
                if skill and len(skill.strip()) > 1:  # è¿‡æ»¤ç©ºæŠ€èƒ½
                    skill_node = f"SKILL_{skill.strip()}"
                    self.skill_nodes.add(skill_node)
                    
                    # è®°å½•è¾¹çš„æƒé‡
                    edge = (skill_node, job_node)
                    edge_weights[edge] += 1
        
        # æ„å»ºå®Œæ•´å›¾
        for (skill_node, job_node), weight in edge_weights.items():
            self.full_graph.add_edge(skill_node, job_node, weight=weight)
            self.all_positive_edges.append((skill_node, job_node))
        
        # ä¸ºèŠ‚ç‚¹æ·»åŠ ç±»å‹å±æ€§
        for skill_node in self.skill_nodes:
            self.full_graph.add_node(skill_node, node_type='skill')
        for job_node in self.job_nodes:
            self.full_graph.add_node(job_node, node_type='job')
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_nodes': len(self.skill_nodes) + len(self.job_nodes),
            'skill_nodes': len(self.skill_nodes),
            'job_nodes': len(self.job_nodes),
            'total_edges': len(self.all_positive_edges),
            'avg_degree': np.mean([self.full_graph.degree(node) for node in self.full_graph.nodes()]),
            'max_degree': max([self.full_graph.degree(node) for node in self.full_graph.nodes()]),
            'density': nx.density(self.full_graph)
        }
        
        print(f"\nğŸ“Š çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print(f"æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']} ({stats['skill_nodes']}ä¸ªæŠ€èƒ½ + {stats['job_nodes']}ä¸ªèŒä¸š)")
        print(f"æ€»è¾¹æ•°: {stats['total_edges']}")
        print(f"å¹³å‡åº¦æ•°: {stats['avg_degree']:.2f}")
        print(f"æœ€å¤§åº¦æ•°: {stats['max_degree']}")
        print(f"å›¾å¯†åº¦: {stats['density']:.6f}")
        
        return stats
    
    def split_positive_edges(self, train_ratio: float = 0.55, val_ratio: float = 0.15, test_ratio: float = 0.30):
        """
        æŒ‰ç…§è®ºæ–‡æ¯”ä¾‹åˆ’åˆ†æ­£æ ·æœ¬è¾¹
        
        Args:
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹  
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        """
        print(f"\næ­£åœ¨åˆ’åˆ†æ­£æ ·æœ¬è¾¹...")
        
        total_edges = len(self.all_positive_edges)
        
        # è®¡ç®—å„é›†åˆå¤§å°
        train_size = int(total_edges * train_ratio)
        val_size = int(total_edges * val_ratio)
        test_size = total_edges - train_size - val_size  # ç¡®ä¿æ€»æ•°æ­£ç¡®
        
        print(f"æ€»æ­£æ ·æœ¬è¾¹: {total_edges}")
        print(f"è®­ç»ƒé›†: {train_size} ({train_size/total_edges*100:.1f}%)")
        print(f"éªŒè¯é›†: {val_size} ({val_size/total_edges*100:.1f}%)")
        print(f"æµ‹è¯•é›†: {test_size} ({test_size/total_edges*100:.1f}%)")
        
        # éšæœºæ‰“ä¹±è¾¹åˆ—è¡¨
        shuffled_edges = self.all_positive_edges.copy()
        random.shuffle(shuffled_edges)
        
        # åˆ’åˆ†æ•°æ®é›†
        self.train_positive = shuffled_edges[:train_size]
        self.val_positive = shuffled_edges[train_size:train_size + val_size]
        self.test_positive = shuffled_edges[train_size + val_size:]
        
        # æ„å»ºè®­ç»ƒå›¾ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†çš„è¾¹ï¼‰
        self.training_graph.clear()
        for skill_node in self.skill_nodes:
            self.training_graph.add_node(skill_node, node_type='skill')
        for job_node in self.job_nodes:
            self.training_graph.add_node(job_node, node_type='job')
        
        for edge in self.train_positive:
            weight = self.full_graph[edge[0]][edge[1]]['weight']
            self.training_graph.add_edge(edge[0], edge[1], weight=weight)
        
        print(f"âœ“ è®­ç»ƒå›¾æ„å»ºå®Œæˆ: {self.training_graph.number_of_nodes()} èŠ‚ç‚¹, {self.training_graph.number_of_edges()} è¾¹")
        
        return {
            'train_positive': len(self.train_positive),
            'val_positive': len(self.val_positive),
            'test_positive': len(self.test_positive),
            'training_graph_edges': self.training_graph.number_of_edges()
        }
    
    def generate_negative_samples(self):
        """
        ç”Ÿæˆè´Ÿæ ·æœ¬è¾¹
        """
        print(f"\næ­£åœ¨ç”Ÿæˆè´Ÿæ ·æœ¬è¾¹...")
        
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„æŠ€èƒ½-èŒä¸šé…å¯¹
        total_possible_pairs = len(self.skill_nodes) * len(self.job_nodes)
        existing_pairs = set(self.all_positive_edges)
        
        print(f"æ€»å¯èƒ½é…å¯¹æ•°: {total_possible_pairs:,}")
        print(f"å·²å­˜åœ¨è¾¹æ•°: {len(existing_pairs):,}")
        print(f"è´Ÿæ ·æœ¬å€™é€‰æ± å¤§å°: {total_possible_pairs - len(existing_pairs):,}")
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬å€™é€‰æ± 
        negative_candidates = []
        for skill_node in self.skill_nodes:
            for job_node in self.job_nodes:
                if (skill_node, job_node) not in existing_pairs:
                    negative_candidates.append((skill_node, job_node))
        
        # æ‰“ä¹±å€™é€‰æ± 
        random.shuffle(negative_candidates)
        
        # ä¸ºå„æ•°æ®é›†ç”Ÿæˆè´Ÿæ ·æœ¬
        train_neg_size = len(self.train_positive)
        val_neg_size = len(self.val_positive)
        test_neg_size = len(self.test_positive)
        
        self.train_negative = negative_candidates[:train_neg_size]
        self.val_negative = negative_candidates[train_neg_size:train_neg_size + val_neg_size]
        self.test_negative = negative_candidates[train_neg_size + val_neg_size:train_neg_size + val_neg_size + test_neg_size]
        
        print(f"âœ“ è´Ÿæ ·æœ¬ç”Ÿæˆå®Œæˆ:")
        print(f"  è®­ç»ƒé›†è´Ÿæ ·æœ¬: {len(self.train_negative)}")
        print(f"  éªŒè¯é›†è´Ÿæ ·æœ¬: {len(self.val_negative)}")
        print(f"  æµ‹è¯•é›†è´Ÿæ ·æœ¬: {len(self.test_negative)}")
        
        # éªŒè¯è´Ÿæ ·æœ¬ç¡®å®ä¸å­˜åœ¨
        for neg_edge in self.train_negative + self.val_negative + self.test_negative:
            assert neg_edge not in existing_pairs, f"è´Ÿæ ·æœ¬ {neg_edge} åœ¨æ­£æ ·æœ¬ä¸­å­˜åœ¨ï¼"
        
        print("âœ“ è´Ÿæ ·æœ¬éªŒè¯é€šè¿‡")
        
        return {
            'total_possible_pairs': total_possible_pairs,
            'negative_candidates': len(negative_candidates),
            'train_negative': len(self.train_negative),
            'val_negative': len(self.val_negative),
            'test_negative': len(self.test_negative)
        }
    
    def calculate_preferential_attachment_scores(self):
        """
        è®¡ç®—ä¼˜å…ˆè¿æ¥ï¼ˆPreferential Attachmentï¼‰å¾—åˆ†
        """
        print(f"\næ­£åœ¨è®¡ç®—ä¼˜å…ˆè¿æ¥å¾—åˆ†...")
        
        # è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹åœ¨è®­ç»ƒå›¾ä¸­çš„åº¦æ•°
        node_degrees = {}
        for node in self.training_graph.nodes():
            node_degrees[node] = self.training_graph.degree(node)
        
        print(f"èŠ‚ç‚¹åº¦æ•°ç»Ÿè®¡:")
        skill_degrees = [degree for node, degree in node_degrees.items() if node.startswith('SKILL_')]
        job_degrees = [degree for node, degree in node_degrees.items() if node.startswith('JOB_')]
        
        print(f"  æŠ€èƒ½èŠ‚ç‚¹åº¦æ•°: å¹³å‡{np.mean(skill_degrees):.2f}, æœ€å¤§{max(skill_degrees) if skill_degrees else 0}")
        print(f"  èŒä¸šèŠ‚ç‚¹åº¦æ•°: å¹³å‡{np.mean(job_degrees):.2f}, æœ€å¤§{max(job_degrees) if job_degrees else 0}")
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„PAå¾—åˆ†
        all_samples = []
        all_samples.extend([(edge, 1) for edge in self.train_positive])  # æ­£æ ·æœ¬æ ‡è®°ä¸º1
        all_samples.extend([(edge, 0) for edge in self.train_negative])  # è´Ÿæ ·æœ¬æ ‡è®°ä¸º0
        all_samples.extend([(edge, 1) for edge in self.val_positive])
        all_samples.extend([(edge, 0) for edge in self.val_negative])
        all_samples.extend([(edge, 1) for edge in self.test_positive])
        all_samples.extend([(edge, 0) for edge in self.test_negative])
        
        # è®¡ç®—PAå¾—åˆ†
        pa_scores = []
        for (u, v), label in all_samples:
            degree_u = node_degrees.get(u, 0)
            degree_v = node_degrees.get(v, 0)
            pa_score = degree_u * degree_v
            pa_scores.append(pa_score)
        
        # å½’ä¸€åŒ–å¾—åˆ†
        max_pa_score = max(pa_scores) if pa_scores else 1
        normalized_scores = [score / max_pa_score for score in pa_scores]
        
        # ä¿å­˜å¾—åˆ†
        self.scores = {
            'samples': all_samples,
            'pa_scores': pa_scores,
            'normalized_scores': normalized_scores,
            'max_pa_score': max_pa_score
        }
        
        print(f"âœ“ PAå¾—åˆ†è®¡ç®—å®Œæˆ:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(all_samples):,}")
        print(f"  PAå¾—åˆ†èŒƒå›´: 0 - {max_pa_score}")
        print(f"  å½’ä¸€åŒ–å¾—åˆ†èŒƒå›´: 0.0 - 1.0")
        print(f"  å¹³å‡PAå¾—åˆ†: {np.mean(pa_scores):.2f}")
        print(f"  å¹³å‡å½’ä¸€åŒ–å¾—åˆ†: {np.mean(normalized_scores):.4f}")
        
        return {
            'total_samples': len(all_samples),
            'max_pa_score': max_pa_score,
            'mean_pa_score': np.mean(pa_scores),
            'mean_normalized_score': np.mean(normalized_scores)
        }
    
    def evaluate_link_prediction(self):
        """
        è¯„ä¼°é“¾æ¥é¢„æµ‹æ€§èƒ½
        """
        print(f"\næ­£åœ¨è¯„ä¼°é“¾æ¥é¢„æµ‹æ€§èƒ½...")
        
        # å‡†å¤‡æ•°æ®
        samples = self.scores['samples']
        predictions = self.scores['normalized_scores']
        
        # åˆ†ç¦»å„æ•°æ®é›†
        train_size = len(self.train_positive) + len(self.train_negative)
        val_size = len(self.val_positive) + len(self.val_negative)
        test_size = len(self.test_positive) + len(self.test_negative)
        
        # è®­ç»ƒé›†
        train_labels = [label for (edge, label) in samples[:train_size]]
        train_predictions = predictions[:train_size]
        
        # éªŒè¯é›†
        val_labels = [label for (edge, label) in samples[train_size:train_size + val_size]]
        val_predictions = predictions[train_size:train_size + val_size]
        
        # æµ‹è¯•é›†
        test_labels = [label for (edge, label) in samples[train_size + val_size:]]
        test_predictions = predictions[train_size + val_size:]
        
        # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
        def calculate_metrics(y_true, y_scores, threshold=0.5):
            y_pred = (np.array(y_scores) >= threshold).astype(int)
            
            return {
                'precision': precision_score(y_true, y_pred),
                'recall': recall_score(y_true, y_pred),
                'f1_score': f1_score(y_true, y_pred),
                'auc_roc': roc_auc_score(y_true, y_scores),
                'auc_pr': average_precision_score(y_true, y_scores)
            }
        
        # è®¡ç®—å„æ•°æ®é›†çš„æ€§èƒ½
        train_metrics = calculate_metrics(train_labels, train_predictions)
        val_metrics = calculate_metrics(val_labels, val_predictions)
        test_metrics = calculate_metrics(test_labels, test_predictions)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š é“¾æ¥é¢„æµ‹æ€§èƒ½è¯„ä¼°ç»“æœ:")
        print(f"{'æŒ‡æ ‡':<15} {'è®­ç»ƒé›†':<10} {'éªŒè¯é›†':<10} {'æµ‹è¯•é›†':<10}")
        print("-" * 50)
        for metric in ['precision', 'recall', 'f1_score', 'auc_roc', 'auc_pr']:
            print(f"{metric:<15} {train_metrics[metric]:<10.4f} {val_metrics[metric]:<10.4f} {test_metrics[metric]:<10.4f}")
        
        return {
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics
        }
    
    def analyze_graph_properties(self):
        """
        åˆ†æå›¾çš„æ‹“æ‰‘æ€§è´¨
        """
        print(f"\næ­£åœ¨åˆ†æå›¾çš„æ‹“æ‰‘æ€§è´¨...")
        
        # è¿é€šæ€§åˆ†æ
        connected_components = list(nx.connected_components(self.full_graph))
        largest_cc = max(connected_components, key=len)
        
        # åº¦åˆ†å¸ƒåˆ†æ
        degrees = [self.full_graph.degree(node) for node in self.full_graph.nodes()]
        skill_degrees = [self.full_graph.degree(node) for node in self.skill_nodes]
        job_degrees = [self.full_graph.degree(node) for node in self.job_nodes]
        
        # ä¸­å¿ƒæ€§åˆ†æï¼ˆåœ¨æœ€å¤§è¿é€šåˆ†é‡ä¸Šè®¡ç®—ï¼‰
        largest_cc_graph = self.full_graph.subgraph(largest_cc)
        betweenness = nx.betweenness_centrality(largest_cc_graph)
        closeness = nx.closeness_centrality(largest_cc_graph)
        
        properties = {
            'connected_components': len(connected_components),
            'largest_cc_size': len(largest_cc),
            'largest_cc_ratio': len(largest_cc) / len(self.full_graph.nodes()),
            'average_degree': np.mean(degrees),
            'degree_std': np.std(degrees),
            'skill_avg_degree': np.mean(skill_degrees),
            'job_avg_degree': np.mean(job_degrees),
            'max_betweenness': max(betweenness.values()),
            'max_closeness': max(closeness.values())
        }
        
        print(f"ğŸ“ˆ å›¾æ‹“æ‰‘æ€§è´¨åˆ†æ:")
        print(f"  è¿é€šåˆ†é‡æ•°: {properties['connected_components']}")
        print(f"  æœ€å¤§è¿é€šåˆ†é‡å¤§å°: {properties['largest_cc_size']} ({properties['largest_cc_ratio']*100:.1f}%)")
        print(f"  å¹³å‡åº¦æ•°: {properties['average_degree']:.2f} Â± {properties['degree_std']:.2f}")
        print(f"  æŠ€èƒ½èŠ‚ç‚¹å¹³å‡åº¦æ•°: {properties['skill_avg_degree']:.2f}")
        print(f"  èŒä¸šèŠ‚ç‚¹å¹³å‡åº¦æ•°: {properties['job_avg_degree']:.2f}")
        
        return properties
    
    def find_top_nodes(self, top_k: int = 10):
        """
        æ‰¾å‡ºåº¦æ•°æœ€é«˜çš„èŠ‚ç‚¹
        """
        print(f"\næ­£åœ¨åˆ†æé‡è¦èŠ‚ç‚¹...")
        
        # æŒ‰åº¦æ•°æ’åº
        node_degrees = [(node, self.full_graph.degree(node)) for node in self.full_graph.nodes()]
        node_degrees.sort(key=lambda x: x[1], reverse=True)
        
        top_skills = []
        top_jobs = []
        
        for node, degree in node_degrees:
            if node.startswith('SKILL_') and len(top_skills) < top_k:
                skill_name = node.replace('SKILL_', '')
                top_skills.append((skill_name, degree))
            elif node.startswith('JOB_') and len(top_jobs) < top_k:
                job_code = node.replace('JOB_', '')
                top_jobs.append((job_code, degree))
        
        print(f"ğŸ† å‰{top_k}ä¸ªæœ€é‡è¦çš„æŠ€èƒ½ï¼ˆæŒ‰è¿æ¥çš„èŒä¸šæ•°ï¼‰:")
        for i, (skill, degree) in enumerate(top_skills, 1):
            print(f"  {i:2d}. {skill}: è¿æ¥{degree}ä¸ªèŒä¸š")
        
        print(f"\nğŸ† å‰{top_k}ä¸ªæœ€é‡è¦çš„èŒä¸šï¼ˆæŒ‰éœ€è¦çš„æŠ€èƒ½æ•°ï¼‰:")
        for i, (job, degree) in enumerate(top_jobs, 1):
            print(f"  {i:2d}. ISCO {job}: éœ€è¦{degree}ä¸ªæŠ€èƒ½")
        
        return {
            'top_skills': top_skills,
            'top_jobs': top_jobs
        }
    
    def save_results(self, output_prefix: str = 'kg_analysis'):
        """
        ä¿å­˜åˆ†æç»“æœ
        """
        print(f"\næ­£åœ¨ä¿å­˜ç»“æœ...")
        
        # ä¿å­˜å›¾ç»“æ„
        nx.write_gexf(self.full_graph, f"{output_prefix}_full_graph.gexf")
        nx.write_gexf(self.training_graph, f"{output_prefix}_training_graph.gexf")
        
        # ä¿å­˜æ•°æ®é›†åˆ†å‰²
        datasets = {
            'train_positive': self.train_positive,
            'val_positive': self.val_positive,
            'test_positive': self.test_positive,
            'train_negative': self.train_negative,
            'val_negative': self.val_negative,
            'test_negative': self.test_negative
        }
        
        with open(f"{output_prefix}_datasets.json", 'w', encoding='utf-8') as f:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_datasets = {k: [list(edge) for edge in v] for k, v in datasets.items()}
            json.dump(serializable_datasets, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯„åˆ†ç»“æœ
        scores_data = {
            'pa_scores': self.scores['pa_scores'],
            'normalized_scores': self.scores['normalized_scores'],
            'max_pa_score': self.scores['max_pa_score']
        }
        
        with open(f"{output_prefix}_scores.json", 'w', encoding='utf-8') as f:
            json.dump(scores_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ç»“æœå·²ä¿å­˜:")
        print(f"  - {output_prefix}_full_graph.gexf: å®Œæ•´çŸ¥è¯†å›¾è°±")
        print(f"  - {output_prefix}_training_graph.gexf: è®­ç»ƒå›¾")
        print(f"  - {output_prefix}_datasets.json: æ•°æ®é›†åˆ†å‰²")
        print(f"  - {output_prefix}_scores.json: PAè¯„åˆ†ç»“æœ")

def main():
    """
    ä¸»å‡½æ•°ï¼šå®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºå’Œé“¾æ¥é¢„æµ‹æµç¨‹
    """
    print("=" * 80)
    print("æŠ€èƒ½-èŒä¸šçŸ¥è¯†å›¾è°±æ„å»ºä¸é“¾æ¥é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 80)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    kg_builder = SkillJobKnowledgeGraph(
        csv_file='simplified_jobs_skills.csv',  # è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        random_seed=42
    )
    
    # æ­¥éª¤1: æ„å»ºçŸ¥è¯†å›¾è°±
    print("\n" + "="*60)
    print("æ­¥éª¤1: æ„å»ºçŸ¥è¯†å›¾è°±")
    print("="*60)
    graph_stats = kg_builder.load_data_and_build_graph()
    
    # æ­¥éª¤2: åˆ’åˆ†æ­£æ ·æœ¬è¾¹
    print("\n" + "="*60)
    print("æ­¥éª¤2: åˆ’åˆ†æ­£æ ·æœ¬è¾¹")
    print("="*60)
    split_stats = kg_builder.split_positive_edges()
    
    # æ­¥éª¤3: ç”Ÿæˆè´Ÿæ ·æœ¬è¾¹
    print("\n" + "="*60)
    print("æ­¥éª¤3: ç”Ÿæˆè´Ÿæ ·æœ¬è¾¹")
    print("="*60)
    negative_stats = kg_builder.generate_negative_samples()
    
    # æ­¥éª¤4: è®¡ç®—ä¼˜å…ˆè¿æ¥å¾—åˆ†
    print("\n" + "="*60)
    print("æ­¥éª¤4: è®¡ç®—ä¼˜å…ˆè¿æ¥å¾—åˆ†")
    print("="*60)
    scoring_stats = kg_builder.calculate_preferential_attachment_scores()
    
    # æ­¥éª¤5: è¯„ä¼°é“¾æ¥é¢„æµ‹æ€§èƒ½
    print("\n" + "="*60)
    print("æ­¥éª¤5: è¯„ä¼°é“¾æ¥é¢„æµ‹æ€§èƒ½")
    print("="*60)
    eval_results = kg_builder.evaluate_link_prediction()
    
    # æ­¥éª¤6: åˆ†æå›¾æ€§è´¨
    print("\n" + "="*60)
    print("æ­¥éª¤6: åˆ†æå›¾æ‹“æ‰‘æ€§è´¨")
    print("="*60)
    graph_properties = kg_builder.analyze_graph_properties()
    
    # æ­¥éª¤7: æ‰¾å‡ºé‡è¦èŠ‚ç‚¹
    print("\n" + "="*60)
    print("æ­¥éª¤7: åˆ†æé‡è¦èŠ‚ç‚¹")
    print("="*60)
    top_nodes = kg_builder.find_top_nodes(top_k=15)
    
    # æ­¥éª¤8: ä¿å­˜ç»“æœ
    print("\n" + "="*60)
    print("æ­¥éª¤8: ä¿å­˜åˆ†æç»“æœ")
    print("="*60)
    kg_builder.save_results('skill_job_kg')
    
    # ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºä¸é“¾æ¥é¢„æµ‹å®Œæˆï¼")
    print("="*80)
    
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  çŸ¥è¯†å›¾è°±è§„æ¨¡: {graph_stats['total_nodes']}ä¸ªèŠ‚ç‚¹, {graph_stats['total_edges']}æ¡è¾¹")
    print(f"  æŠ€èƒ½èŠ‚ç‚¹: {graph_stats['skill_nodes']}ä¸ª")
    print(f"  èŒä¸šèŠ‚ç‚¹: {graph_stats['job_nodes']}ä¸ª")
    print(f"  å›¾å¯†åº¦: {graph_stats['density']:.6f}")
    
    print(f"\nğŸ”¬ æ•°æ®é›†åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {split_stats['train_positive']}æ¡æ­£æ ·æœ¬ + {negative_stats['train_negative']}æ¡è´Ÿæ ·æœ¬")
    print(f"  éªŒè¯é›†: {split_stats['val_positive']}æ¡æ­£æ ·æœ¬ + {negative_stats['val_negative']}æ¡è´Ÿæ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {split_stats['test_positive']}æ¡æ­£æ ·æœ¬ + {negative_stats['test_negative']}æ¡è´Ÿæ ·æœ¬")
    
    print(f"\nğŸ¯ é“¾æ¥é¢„æµ‹æ€§èƒ½ (æµ‹è¯•é›†):")
    test_metrics = eval_results['test_metrics']
    print(f"  F1-Score: {test_metrics['f1_score']:.4f}")
    print(f"  AUC-ROC: {test_metrics['auc_roc']:.4f}")
    print(f"  AUC-PR: {test_metrics['auc_pr']:.4f}")
    
    print(f"\nğŸ’¡ åç»­ç ”ç©¶æ–¹å‘:")
    print(f"  - å°è¯•å…¶ä»–é“¾æ¥é¢„æµ‹ç®—æ³• (Node2Vec, GraphSAGEç­‰)")
    print(f"  - åˆ†æèŒä¸šè½¬æ¢è·¯å¾„")
    print(f"  - æ„å»ºæŠ€èƒ½æ¨èç³»ç»Ÿ")
    print(f"  - è¿›è¡Œè¡Œä¸šæŠ€èƒ½ç”»åƒåˆ†æ")

if __name__ == "__main__":
    main()