import pandas as pd
import jieba
import jieba.posseg as pseg
import re
from collections import Counter, defaultdict
import numpy as np
import json
from typing import List, Dict, Set, Tuple
import warnings
from fuzzywuzzy import fuzz, process

warnings.filterwarnings('ignore')

class ImprovedSkillExtractor:
    def __init__(self):
        """æ”¹è¿›ç‰ˆæŠ€èƒ½æå–å™¨"""
        # æ ¸å¿ƒæŠ€èƒ½è¯å…¸
        self.skill_keywords = self._load_core_skills()
        
        # åŒä¹‰è¯æ˜ å°„
        self.synonyms_dict = self._load_synonyms()
        
        # åœç”¨è¯å’Œå™ªå£°è¯
        self.stop_words = self._load_stop_words()
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.skill_patterns = self._compile_skill_patterns()
        
        # æ·»åŠ è‡ªå®šä¹‰è¯å…¸
        self._add_custom_dict()
        
        print("æ”¹è¿›ç‰ˆæŠ€èƒ½æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_core_skills(self) -> Dict[str, List[str]]:
        """åŠ è½½æ ¸å¿ƒæŠ€èƒ½è¯å…¸ï¼ˆæ•´åˆäººå·¥æ€»ç»“çš„æŠ€èƒ½ï¼‰"""
        return {
            # ç¼–ç¨‹ä¸å¼€å‘
            'programming': [
                'Python', 'Java', 'JavaScript', 'C++', 'PHP', 'Go', 'C#', 'Swift', 'Objective-C',
                'HTML', 'CSS', 'React', 'Vue', 'Angular', 'Node.js', 'Django', 'Flask',
                'J2EE', 'Hibernate', 'iBatis', 'Spring', 'Struts2', 'Eclipse',
                '.NET', 'ASP.NET', 'jQuery', 'Ajax', 'Web Service',
                'C/C++', 'Cocos2d-x', 'Androidå¼€å‘', 'iOSå¼€å‘', 'APPå¼€å‘',
                'MFC/WTLæ¡†æ¶', 'ActiveXç»„ä»¶', 'DirectX', 'Windows GDI'
            ],
            
            # æ•°æ®åº“æŠ€æœ¯
            'database': [
                'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQL Server', 'DB2',
                'æ•°æ®åº“åº”ç”¨å¼€å‘', 'æ•°æ®åº“è®¾è®¡', 'SQL', 'NoSQL'
            ],
            
            # ç³»ç»Ÿä¸è¿ç»´
            'system_ops': [
                'Linux', 'Git', 'Docker', 'SVN', 'JIRA', 'Confluence',
                'Tomcat', 'Jboss', 'WebLogic', 'WebSphere',
                'ç½‘ç»œç»¼åˆå¸ƒçº¿', 'æœåŠ¡å™¨é…ç½®', 'äº¤æ¢æœºè°ƒè¯•', 'é˜²ç«å¢™è°ƒè¯•',
                'ITè¿ç»´', 'æœºæˆ¿ç®¡ç†', 'ç½‘ç»œç›‘æ§', 'æ•°æ®å¤‡ä»½', 'æ•…éšœæ’é™¤'
            ],
            
            # åŠå…¬è½¯ä»¶
            'office_tools': [
                'Excel', 'Word', 'PowerPoint', 'PPT', 'Office', 'Outlook',
                'Photoshop', 'PS', 'Illustrator', 'AI', 'CorelDRAW', 'Dreamweaver', 'Flash',
                'InDesign', '3DMAX', 'Maya', 'AutoCAD', 'CAD', 'Pro/E', 'SolidWorks',
                'Catia', 'Inventor', 'VRay', 'SketchUp', 'SAI',
                'WPS', 'Visio', 'Project'
            ],
            
            # ä¼ä¸šè½¯ä»¶
            'enterprise_software': [
                'ç”¨å‹U8', 'SAP', 'K3', 'é‡‘è¶', 'ERPç³»ç»Ÿ', 'MESç³»ç»Ÿ', 'CRM',
                'OAåŠå…¬è‡ªåŠ¨åŒ–ç³»ç»Ÿ', 'SPSS', 'SAS', 'å¹¿è”è¾¾è½¯ä»¶'
            ],
            
            # è¯­è¨€èƒ½åŠ›
            'languages': [
                'è‹±è¯­', 'è‹±è¯­å¬è¯´è¯»å†™', 'å¾·è¯­', 'æ—¥è¯­', 'æ™®é€šè¯', 'ç²¤è¯­', 'è‹±æ–‡', 'æ—¥æ–‡', 'éŸ©æ–‡',
                'CET-4', 'CET-6', 'å››çº§', 'å…­çº§', 'é›…æ€', 'æ‰˜ç¦', 'TOEFL', 'IELTS',
                'TESOL'
            ],
            
            # ä¸“ä¸šè¯ä¹¦
            'certificates': [
                'PMP', 'CPA', 'CFA', 'æ³¨å†Œä¼šè®¡å¸ˆ', 'ä¸­çº§ä¼šè®¡å¸ˆ', 'åˆçº§ä¼šè®¡å¸ˆ',
                'æ‰§ä¸šè¯å¸ˆèµ„æ ¼è¯', 'å¾‹å¸ˆæ‰§ä¸šèµ„æ ¼è¯', 'ä¼šè®¡ä»ä¸šèµ„æ ¼è¯',
                'æŠ¥å…³å‘˜èµ„æ ¼è¯', 'æŠ¥æ£€å‘˜èµ„æ ¼è¯', 'é©¾é©¶è¯', 'Aç…§', 'Bç…§', 'Cç…§',
                'ç”µå·¥è¯', 'ç„Šå·¥è¯', 'æ•™å¸ˆèµ„æ ¼è¯', 'äººåŠ›èµ„æºç®¡ç†å¸ˆ',
                'é€ ä»·å‘˜èµ„æ ¼è¯', 'å·¥ç¨‹å¸ˆèŒç§°', 'å»ºé€ å¸ˆè¯ä¹¦', 'å®‰å…¨å‘˜è¯',
                'AFP', 'CFP', 'è¯åˆ¸ä»ä¸šèµ„æ ¼è¯', 'ä¿é™©ä»£ç†äººèµ„æ ¼è¯'
            ],
            
            # æ²Ÿé€šä¸è¡¨è¾¾æŠ€èƒ½
            'communication_skills': [
                'æ²Ÿé€šèƒ½åŠ›', 'äº¤æµèƒ½åŠ›', 'è¡¨è¾¾èƒ½åŠ›', 'æ¼”è®²èƒ½åŠ›', 'æ±‡æŠ¥èƒ½åŠ›',
                'å€¾å¬èƒ½åŠ›', 'è°ˆåˆ¤èƒ½åŠ›', 'å•†åŠ¡æ²Ÿé€š', 'æ–‡å­—åŠŸåº•', 'å†™ä½œèƒ½åŠ›',
                'å£é½¿æ¸…æ™°', 'å–„äºå€¾å¬', 'å…¬æ–‡å†™ä½œ'
            ],
            
            # é¢†å¯¼ä¸ç®¡ç†æŠ€èƒ½
            'leadership_skills': [
                'é¢†å¯¼åŠ›', 'é¢†å¯¼èƒ½åŠ›', 'ç®¡ç†èƒ½åŠ›', 'å›¢é˜Ÿç®¡ç†', 'äººå‘˜ç®¡ç†',
                'é¡¹ç›®ç®¡ç†', 'å›¢é˜Ÿå»ºè®¾', 'å†³ç­–èƒ½åŠ›', 'ç»„ç»‡èƒ½åŠ›', 'è®¡åˆ’èƒ½åŠ›',
                'æ§åˆ¶èƒ½åŠ›', 'æ‰§è¡Œèƒ½åŠ›', 'è½¯ä»¶é¡¹ç›®ç®¡ç†'
            ],
            
            # å›¢é˜Ÿåä½œæŠ€èƒ½
            'teamwork_skills': [
                'å›¢é˜Ÿåˆä½œ', 'åä½œèƒ½åŠ›', 'åè°ƒèƒ½åŠ›', 'é…åˆèƒ½åŠ›', 'å›¢é˜Ÿç²¾ç¥',
                'å›¢é˜Ÿåˆä½œç²¾ç¥', 'ç‹¬ç«‹å·¥ä½œèƒ½åŠ›'
            ],
            
            # åˆ†æä¸æ€ç»´æŠ€èƒ½
            'analytical_skills': [
                'åˆ†æèƒ½åŠ›', 'é€»è¾‘æ€ç»´', 'æ•°æ®åˆ†æ', 'è´¢åŠ¡åˆ†æ', 'å¸‚åœºåˆ†æ',
                'é—®é¢˜åˆ†æ', 'ç»Ÿè®¡åˆ†æ', 'åˆ¤æ–­èƒ½åŠ›', 'é—®é¢˜è§£å†³èƒ½åŠ›',
                'é€»è¾‘æ€ç»´èƒ½åŠ›', 'æ´å¯ŸåŠ›', 'å½’çº³æ€»ç»“èƒ½åŠ›'
            ],
            
            # ä¸ªäººç´ è´¨æŠ€èƒ½
            'personal_skills': [
                'å­¦ä¹ èƒ½åŠ›', 'é€‚åº”èƒ½åŠ›', 'æŠ—å‹èƒ½åŠ›', 'åˆ›æ–°èƒ½åŠ›', 'æ‰§è¡ŒåŠ›',
                'è´£ä»»å¿ƒ', 'ä¸»åŠ¨æ€§', 'æ—¶é—´ç®¡ç†', 'è‡ªæˆ‘ç®¡ç†', 'åº”å˜èƒ½åŠ›',
                'ç§¯æä¸»åŠ¨', 'åƒè‹¦è€åŠ³', 'è‡ªæˆ‘æ¿€åŠ±', 'äº²å’ŒåŠ›', 'è€å¿ƒ', 'ç»†å¿ƒ',
                'æ—¶é—´ç®¡ç†èƒ½åŠ›', 'æ•¬ä¸šç²¾ç¥', 'èŒä¸šé“å¾·', 'æœ‰æ¿€æƒ…', 'åŒç†å¿ƒ',
                'æ²‰ç¨³', 'éšå¿', 'ä¹è§‚', 'è‡ªå¾‹æ€§'
            ],
            
            # å®¢æˆ·æœåŠ¡æŠ€èƒ½
            'service_skills': [
                'å®¢æˆ·æœåŠ¡', 'å®¢æˆ·æœåŠ¡æ„è¯†', 'æœåŠ¡æ„è¯†', 'å®¢æˆ·å…³ç³»ç®¡ç†',
                'å®¢æˆ·å¼€å‘', 'å®¢æˆ·éœ€æ±‚æŒ–æ˜', 'å®¢æˆ·ä¿¡æ¯æ”¶é›†', 'å”®åæœåŠ¡'
            ],
            
            # å¸‚åœºè¥é”€æŠ€èƒ½
            'marketing_skills': [
                'å¸‚åœºè¥é”€', 'å¸‚åœºè¥é”€å­¦', 'å“ç‰Œè¿ä½œ', 'æ¸ é“ç»´æŠ¤ç®¡ç†', 'ç½‘ç»œæ¨å¹¿',
                'æœç´¢å¼•æ“ç«ä»·', 'ç™¾åº¦æ¨å¹¿', 'Googleæ¨å¹¿', 'SEO', 'SEM',
                'B2Bå¹³å°æ“ä½œ', 'B2Cå¹³å°æ“ä½œ', 'å¤–è´¸å¹³å°æ“ä½œ', 'åº—é“ºå»ºè®¾',
                'äº§å“æ¨å¹¿', 'ç”µè¯è¥é”€', 'ç½‘ç»œè¥é”€', 'å¸‚åœºè°ƒç ”', 'æ‹›å•†ç®¡ç†',
                'ä¿ƒé”€ç­–åˆ’', 'å¹¿å‘Šç­–åˆ’', 'åª’ä½“æ¨å¹¿', 'å…¬å…³æ´»åŠ¨ç­–åˆ’',
                'æ•´åˆè¥é”€', 'æ•´åˆæ¨å¹¿', 'ç”µå­å•†åŠ¡', 'ç½‘ç»œé”€å”®'
            ],
            
            # é”€å”®æŠ€èƒ½
            'sales_skills': [
                'é”€å”®', 'é”€å”®æŠ€å·§', 'å®¢æˆ·å¼€å‘', 'æ½œåœ¨å®¢æˆ·å¼€å‘', 'é”€å”®æ•°æ®åˆ†æ',
                'å¸‚åœºæ‹“å±•', 'KAç³»ç»Ÿæ“ä½œ', 'ç»é”€å•†ç®¡ç†', 'ç»ˆç«¯ç®¡ç†',
                'æŠ•æ ‡å•†åŠ¡æ–‡ä»¶ç¼–åˆ¶', 'åˆåŒèµ·è‰', 'æˆ¿äº§ç»çºª', 'ç½®ä¸šå’¨è¯¢',
                'æˆ¿å±‹è¿‡æˆ·æ‰‹ç»­åŠç†', 'æ¡ˆåœºæ¥å¾…', 'é”€è®²', 'ç­”å®¢é—®', 'ä½™æ¬¾å‚¬ç¼´'
            ],
            
            # äººåŠ›èµ„æºæŠ€èƒ½
            'hr_skills': [
                'äººåŠ›èµ„æºç®¡ç†', 'æ‹›è˜æµç¨‹ç®¡ç†', 'å…¥èŒæ‰‹ç»­åŠç†', 'ç¦»èŒæ‰‹ç»­åŠç†',
                'åŠ³åŠ¨åˆåŒç®¡ç†', 'è–ªé…¬ç®¡ç†', 'ç»©æ•ˆè€ƒæ ¸', 'åŸ¹è®­ä¸å‘å±•',
                'åŸ¹è®­éœ€æ±‚åˆ†æ', 'åŸ¹è®­è®¡åˆ’åˆ¶å®š', 'åŸ¹è®­æ•™æç¼–å†™', 'åŸ¹è®­æ•ˆæœè¯„ä¼°',
                'å‘˜å·¥å…³ç³»ç®¡ç†', 'ä¼ä¸šæ–‡åŒ–å»ºè®¾'
            ],
            
            # è¡Œæ”¿ç®¡ç†æŠ€èƒ½
            'admin_skills': [
                'å‰å°æ¥å¾…æŠ€å·§', 'ç”µè¯æ¥è½¬æŠ€å·§', 'æ–‡ä»¶ç®¡ç†', 'æ¡£æ¡ˆç®¡ç†',
                'ä¼šè®®ç®¡ç†', 'åŠå…¬ç”¨å“ç®¡ç†', 'èµ„äº§ç®¡ç†', 'è€ƒå‹¤ç®¡ç†',
                'å‘˜å·¥é€šè®¯ä¿¡æ¯ç®¡ç†', 'åå‹¤ä¿éšœ', 'è¡Œæ”¿äº‹åŠ¡å¤„ç†', 'å•†åŠ¡ç¤¼ä»ª'
            ],
            
            # è´¢åŠ¡ä¼šè®¡æŠ€èƒ½
            'finance_skills': [
                'è´¢åŠ¡é¢„ç®—', 'ä¼šè®¡æ ¸ç®—', 'æˆæœ¬æ ¸ç®—', 'æˆæœ¬æ§åˆ¶', 'è´¢åŠ¡æŠ¥è¡¨ç¼–åˆ¶',
                'è´¢åŠ¡åˆ†æ', 'ç¨åŠ¡ç”³æŠ¥', 'å‡ºå£é€€ç¨', 'è½¯ä»¶äº§å“é€€ç¨', 'æ€»è´¦å¤„ç†',
                'åº”æ”¶åº”ä»˜è´¦æ¬¾ç®¡ç†', 'é“¶è¡Œç»“ç®—ä¸šåŠ¡', 'ç°é‡‘ç®¡ç†', 'ç¥¨æ®ç®¡ç†',
                'å†…éƒ¨å®¡è®¡', 'èµ„é‡‘ç®¡ç†', 'èèµ„ç®¡ç†', 'æŠ•èµ„ç®¡ç†', 'ç¨åŠ¡ç­¹åˆ’',
                'é£é™©æ§åˆ¶èƒ½åŠ›', 'æˆæœ¬æ„è¯†'
            ],
            
            # ç”Ÿäº§åˆ¶é€ æŠ€èƒ½
            'production_skills': [
                'ç”Ÿäº§è®¡åˆ’', 'ç”Ÿäº§è°ƒåº¦', 'ç”Ÿäº§ç®¡ç†', 'è´¨é‡ç®¡ç†', 'å“è´¨æ£€éªŒ',
                'IQC', 'PQC', 'å·¥è‰ºå¼€å‘', 'å·¥è‰ºæµç¨‹è§„åˆ’', 'ç°åœºç®¡ç†',
                'è®¾å¤‡ç®¡ç†', 'è®¾å¤‡ç»´æŠ¤ä¿å…»', 'æ¨¡å…·è®¾è®¡', 'æ¨¡å…·ç»´ä¿®',
                'ä½œä¸šæŒ‡å¯¼ä¹¦ç¼–å†™', 'BOMå•ç®¡ç†', 'äº§èƒ½è¯„ä¼°', 'ç„Šæ¥æŠ€æœ¯',
                'CNCç¼–ç¨‹ä¸æ“ä½œ', 'é’³å·¥', 'æœºä¿®'
            ],
            
            # è´¨é‡ç®¡ç†æŠ€èƒ½
            'quality_skills': [
                'ISO9000', 'ISO9001', 'TS16949', 'ISO14001', 'OHSAS18001',
                'GMP', 'HACCP', '6sigma', 'å…­è¥¿æ ¼ç›', '5Sç®¡ç†', '8DæŠ¥å‘Š'
            ],
            
            # ç‰©æµä»“å‚¨æŠ€èƒ½
            'logistics_skills': [
                'ä»“åº“ç®¡ç†', 'åº“å­˜ç®¡ç†', 'è¿›å‡ºåº“ç®¡ç†', 'ç›˜ç‚¹', 'ç‰©æµé…é€',
                'è´§ç‰©è·Ÿè¸ª', 'åˆ¶å•', 'æ ‡ç­¾åˆ¶ä½œ', 'æ‰˜è¿ä¹¦ç®¡ç†',
                'å›½é™…è´§è¿ä»£ç†', 'æµ·è¿æ“ä½œ', 'ç©ºè¿æ“ä½œ', 'æŠ¥å…³', 'æ¸…å…³',
                'å•è¯åˆ¶ä½œ', 'L/Cå®¡å•åˆ¶å•', 'SHIPPINGæµç¨‹æ“ä½œ', 'ä¾›åº”é“¾ç®¡ç†'
            ],
            
            # è®¾è®¡åˆ›æ„æŠ€èƒ½
            'design_skills': [
                'å¹³é¢è®¾è®¡', 'ç½‘é¡µè®¾è®¡', 'ç¾å·¥', 'å›¾ç‰‡å¤„ç†', 'è§†è§‰è®¾è®¡',
                'å¹¿å‘Šè®¾è®¡', 'åŒ…è£…è®¾è®¡', 'æµ·æŠ¥è®¾è®¡', 'å®£ä¼ å†Œè®¾è®¡',
                'åº—é“ºè£…ä¿®', 'è¯¦æƒ…é¡µè®¾è®¡', 'åˆ›æ„æ„æ€', 'è‰²å½©æ­é…',
                'æ‰‹ç»˜èƒ½åŠ›', 'UIè®¾è®¡', 'GUIè®¾è®¡', 'äº¤äº’è®¾è®¡'
            ],
            
            # å¤šåª’ä½“æŠ€èƒ½
            'multimedia_skills': [
                'åŠ¨ç”»åˆ¶ä½œ', 'è§†é¢‘æ‹æ‘„', 'è§†é¢‘å‰ªè¾‘', 'è§†é¢‘åæœŸå¤„ç†',
                'EDIUS', 'Final Cut Pro', 'Premiere Pro', '3Dè§’è‰²åŠ¨ä½œè®¾è®¡',
                'åœºæ™¯åŸç”»è®¾è®¡', 'æ¸¸æˆåŸç”»', 'æ‘„å½±', 'å›¾ç‰‡ä¿®è°ƒ', 'æ’­éŸ³ä¸»æŒ'
            ],
            
            # å·¥ç¨‹å»ºç­‘æŠ€èƒ½
            'engineering_skills': [
                'å·¥ç¨‹é€ ä»·', 'å·¥ç¨‹é¢„ç»“ç®—', 'åœŸæœ¨å·¥ç¨‹', 'å»ºç­‘æ–½å·¥',
                'æ–½å·¥ç»„ç»‡è®¾è®¡', 'å›¾çº¸ä¼šå®¡', 'å·¥ç¨‹éªŒæ”¶', 'å·¥ç¨‹å˜æ›´ç®¡ç†',
                'ç­¾è¯å·¥ä½œ', 'å»ºç­‘å®‰å…¨ç®¡ç†', 'ç»“æ„å·¥ç¨‹æ£€æµ‹', 'å²©åœŸå·¥ç¨‹',
                'æµ‹é‡æ”¾æ ·', 'ç»™æ’æ°´å·¥ç¨‹', 'æš–é€šå·¥ç¨‹', 'ç©ºæ°”å‡€åŒ–å·¥ç¨‹'
            ],
            
            # æ•™è‚²åŸ¹è®­æŠ€èƒ½
            'education_skills': [
                'æ•™å­¦èƒ½åŠ›', 'è¯¾ç¨‹å¼€å‘', 'æ•™æ¡ˆåˆ¶ä½œ', 'æ•™å…·åˆ¶ä½œ', 'è¯¾å ‚ç®¡ç†',
                'å…¬å¼€è¯¾æ¼”è®²', 'å®¶æ ¡äº’åŠ¨', 'å­¦å‘˜ç®¡ç†', 'è¥å…»å¸ˆåŸ¹è®­',
                'è‚²å©´å¸ˆåŸ¹è®­', 'å°å„¿æ¨æ‹¿å¸ˆåŸ¹è®­', 'è¯­æ–‡æ•™å­¦', 'æ•™æç¼–è¾‘',
                'æ ¡å¯¹', 'é’¢ç´æ•™å­¦', 'è§†å¥èƒ½åŠ›', 'å› ææ–½æ•™', 'è‹±è¯­æ•™å­¦'
            ],
            
            # æ³•å¾‹ä¸“ä¸šæŠ€èƒ½
            'legal_skills': [
                'æ³•å¾‹çŸ¥è¯†', 'åˆåŒæ³•', 'å…¬å¸æ³•', 'è¯‰è®¼æ³•', 'çŸ¥è¯†äº§æƒæ³•',
                'ç»æµæ³•', 'æ°‘å•†æ³•', 'åŠ³åŠ¨æ³•', 'å¸æ³•è¯‰è®¼ç¨‹åº', 'æ³•å¾‹é£é™©é˜²èŒƒ',
                'å¾‹å¸ˆæ‰§ä¸š', 'å•†æ ‡å¤„ç†', 'ä¸“åˆ©å¤„ç†'
            ],
            
            # åŒ»ç–—å¥åº·æŠ€èƒ½
            'medical_skills': [
                'ä¸­è¥¿è¯å­¦çŸ¥è¯†', 'æ‰§ä¸šè¯å¸ˆ', 'ä¸´åºŠåŒ»å­¦', 'ç”·ç§‘æ‰‹æœ¯',
                'å£è…”ä¿®å¤æŠ€æœ¯', 'æ ¹ç®¡æ²»ç–—æŠ€æœ¯', 'çš®è‚¤æŠ¤ç†', 'ç¾å®¹ç¾ä½“',
                'ç¾ç”²', 'åŒ–å¦†', 'ä¸­åŒ»å…»ç”Ÿä¿å¥', 'ä¸­è¯ææ€§çŠ¶ç”¨æ³•',
                'ä½“å¤–è¯Šæ–­è¯•å‰‚ç ”å‘', 'ä½“å¤–è¯Šæ–­è¯•å‰‚ç”Ÿäº§'
            ],
            
            # æŠ€æœ¯ä¸“ä¸šæŠ€èƒ½
            'technical_skills': [
                'åµŒå…¥å¼ç³»ç»Ÿè®¾è®¡', 'å•ç‰‡æœº', 'ARMå¤„ç†å™¨', 'DSPæ•°å­—ä¿¡å·å¤„ç†å™¨',
                'FPGA', 'PLCç¼–ç¨‹', 'ç”µæœºä¼ºæœæ§åˆ¶', 'LabVIEWç¼–ç¨‹',
                'ç³»ç»Ÿé›†æˆ', 'ç”¨æˆ·åŸ¹è®­', 'æŠ€æœ¯æ–¹æ¡ˆç¼–å†™', 'å®æ–½æ–¹æ¡ˆç¼–å†™',
                'éªŒæ”¶æ–‡æ¡£ç¼–å†™', 'å¾®æ³¢å¤©çº¿é€šè®¯æŠ€æœ¯', 'å…‰çº¤å…‰æ …åˆ¶ä½œ',
                'å…‰çº¤æ¿€å…‰å™¨è®¾è®¡', 'å…‰å­¦å™¨ä»¶æµ‹è¯•', 'å…‰çº¤ä¼ æ„Ÿç³»ç»Ÿ'
            ],
            
            # å…¶ä»–ä¸“ä¸šæŠ€èƒ½
            'other_professional': [
                'åœŸåœ°æˆ¿äº§ç®¡ç†', 'ç²¾ç®—è¯„ä¼°', 'Prophetæ¨¡å‹', 'èµ„äº§è´Ÿå€ºç®¡ç†',
                'é©¾é©¶æŠ€æœ¯', 'è½¦è¾†ä¿å…»ç»´ä¿®', 'ç”µå·¥æŠ€èƒ½', 'é«˜å‹å…¥ç½‘',
                'ç”µæ¢¯æ“ä½œ', 'æ°´ç”µç»´ä¿®å®‰è£…', 'æ¶ˆé˜²å®‰å…¨ç®¡ç†', 'ç‰©ä¸šç®¡ç†',
                'é¤é¥®ç®¡ç†', 'åå¨ç®¡ç†', 'èœå“å¼€å‘', 'å®´ä¼šæœåŠ¡', 'é¤å…æœåŠ¡',
                'é…’æ°´è°ƒåˆ¶', 'èŒ¶è‰º', 'è¥¿é¤æœåŠ¡', 'é£Ÿå“æ£€éªŒ', 'å¾®ç”Ÿç‰©æ£€éªŒ',
                'é…¿é€ å·¥è‰º', 'ä¿å¯†æ„è¯†', 'å…¨å±€è§‚', 'å¤§å±€è§‚', 'ä½¿å‘½æ„Ÿ'
            ]
        }
    
    def _load_synonyms(self) -> Dict[str, str]:
        """åŠ è½½åŒä¹‰è¯æ˜ å°„ï¼ˆæ•´åˆäººå·¥æ€»ç»“çš„åŒä¹‰è¯ï¼‰"""
        return {
            # æ²Ÿé€šç›¸å…³åŒä¹‰è¯
            'äº¤æµèƒ½åŠ›': 'æ²Ÿé€šèƒ½åŠ›',
            'è¡¨è¾¾èƒ½åŠ›': 'æ²Ÿé€šèƒ½åŠ›',
            'æ²Ÿé€šæŠ€å·§': 'æ²Ÿé€šèƒ½åŠ›',
            'äº¤æµæŠ€å·§': 'æ²Ÿé€šèƒ½åŠ›',
            'è°ˆåˆ¤èƒ½åŠ›': 'æ²Ÿé€šèƒ½åŠ›',
            'å•†åŠ¡æ²Ÿé€š': 'æ²Ÿé€šèƒ½åŠ›',
            'å£é½¿æ¸…æ™°': 'æ²Ÿé€šèƒ½åŠ›',
            'å–„äºå€¾å¬': 'æ²Ÿé€šèƒ½åŠ›',
            
            # å›¢é˜Ÿç›¸å…³åŒä¹‰è¯
            'åä½œèƒ½åŠ›': 'å›¢é˜Ÿåˆä½œ',
            'é…åˆèƒ½åŠ›': 'å›¢é˜Ÿåˆä½œ',
            'å›¢é˜Ÿç²¾ç¥': 'å›¢é˜Ÿåˆä½œ',
            'åˆä½œèƒ½åŠ›': 'å›¢é˜Ÿåˆä½œ',
            'å›¢é˜Ÿåˆä½œç²¾ç¥': 'å›¢é˜Ÿåˆä½œ',
            
            # é¢†å¯¼ç®¡ç†ç›¸å…³åŒä¹‰è¯
            'é¢†å¯¼èƒ½åŠ›': 'é¢†å¯¼åŠ›',
            'ç®¡ç†èƒ½åŠ›': 'é¢†å¯¼åŠ›',
            'ç®¡ç†æŠ€èƒ½': 'é¢†å¯¼åŠ›',
            'å›¢é˜Ÿç®¡ç†': 'é¢†å¯¼åŠ›',
            'äººå‘˜ç®¡ç†': 'é¢†å¯¼åŠ›',
            'ç»„ç»‡èƒ½åŠ›': 'é¢†å¯¼åŠ›',
            'è®¡åˆ’èƒ½åŠ›': 'é¢†å¯¼åŠ›',
            'æ§åˆ¶èƒ½åŠ›': 'é¢†å¯¼åŠ›',
            
            # åˆ†ææ€ç»´ç›¸å…³åŒä¹‰è¯
            'åˆ†ææŠ€èƒ½': 'åˆ†æèƒ½åŠ›',
            'é€»è¾‘åˆ†æ': 'åˆ†æèƒ½åŠ›',
            'æ•°æ®åˆ†æèƒ½åŠ›': 'åˆ†æèƒ½åŠ›',
            'é€»è¾‘æ€ç»´èƒ½åŠ›': 'åˆ†æèƒ½åŠ›',
            'åˆ¤æ–­èƒ½åŠ›': 'åˆ†æèƒ½åŠ›',
            'é—®é¢˜åˆ†æ': 'åˆ†æèƒ½åŠ›',
            'é—®é¢˜è§£å†³èƒ½åŠ›': 'åˆ†æèƒ½åŠ›',
            
            # ä¸ªäººç´ è´¨ç›¸å…³åŒä¹‰è¯
            'æ‰§è¡Œèƒ½åŠ›': 'æ‰§è¡ŒåŠ›',
            'åº”å˜èƒ½åŠ›': 'é€‚åº”èƒ½åŠ›',
            'æŠ—å‹èƒ½åŠ›': 'é€‚åº”èƒ½åŠ›',
            'æ—¶é—´ç®¡ç†èƒ½åŠ›': 'æ—¶é—´ç®¡ç†',
            'è‡ªæˆ‘æ¿€åŠ±': 'ä¸»åŠ¨æ€§',
            'ç§¯æä¸»åŠ¨': 'ä¸»åŠ¨æ€§',
            'èŒä¸šé“å¾·': 'è´£ä»»å¿ƒ',
            'æ•¬ä¸šç²¾ç¥': 'è´£ä»»å¿ƒ',
            
            # æŠ€æœ¯ç›¸å…³åŒä¹‰è¯
            'JS': 'JavaScript',
            'js': 'JavaScript',
            'é¢å‘å¯¹è±¡è®¾è®¡': 'OOD',
            'é¢å‘å¯¹è±¡ç¼–ç¨‹': 'OOP',
            
            # åŠå…¬è½¯ä»¶åŒä¹‰è¯
            'PPT': 'PowerPoint',
            'ppt': 'PowerPoint',
            'PS': 'Photoshop',
            'ps': 'Photoshop',
            'AI': 'Illustrator',
            'office': 'Office',
            'OFFICE': 'Office',
            'Wordæ–‡æ¡£': 'Word',
            'Excelè¡¨æ ¼': 'Excel',
            
            # æ•°æ®åº“åŒä¹‰è¯
            'mysql': 'MySQL',
            'MYSQL': 'MySQL',
            'oracle': 'Oracle',
            'ORACLE': 'Oracle',
            
            # å­¦å†ç›¸å…³åŒä¹‰è¯
            'æœ¬ç§‘ä»¥ä¸Š': 'æœ¬ç§‘',
            'å¤§ä¸“ä»¥ä¸Š': 'å¤§ä¸“',
            'ä¸“ç§‘ä»¥ä¸Š': 'ä¸“ç§‘',
            'ç¡•å£«ä»¥ä¸Š': 'ç¡•å£«',
            'ç ”ç©¶ç”Ÿä»¥ä¸Š': 'ç ”ç©¶ç”Ÿ',
            
            # è¯­è¨€èƒ½åŠ›åŒä¹‰è¯
            'è‹±è¯­å¬è¯´è¯»å†™': 'è‹±è¯­',
            'è‹±æ–‡': 'è‹±è¯­',
            'æ—¥æ–‡': 'æ—¥è¯­',
            'éŸ©æ–‡': 'éŸ©è¯­',
            'å››çº§': 'CET-4',
            'å…­çº§': 'CET-6',
            
            # è¯ä¹¦åŒä¹‰è¯
            'æ³¨å†Œä¼šè®¡å¸ˆ': 'CPA',
            'é¡¹ç›®ç®¡ç†': 'PMP',
            'é©¾ç…§': 'é©¾é©¶è¯',
            'Cç…§': 'é©¾é©¶è¯',
            'Bç…§': 'é©¾é©¶è¯',
            'Aç…§': 'é©¾é©¶è¯',
            
            # ä¸šåŠ¡æŠ€èƒ½åŒä¹‰è¯
            'å¸‚åœºè¥é”€å­¦': 'å¸‚åœºè¥é”€',
            'å®¢æˆ·å…³ç³»ç®¡ç†': 'CRM',
            'å®¢æˆ·æœåŠ¡æ„è¯†': 'å®¢æˆ·æœåŠ¡',
            'æœåŠ¡æ„è¯†': 'å®¢æˆ·æœåŠ¡',
            'ç½‘ç»œæ¨å¹¿': 'ç½‘ç»œè¥é”€',
            'ç”µè¯è¥é”€': 'é”€å”®',
            'ç½‘ç»œé”€å”®': 'é”€å”®',
            
            # è´¨é‡ç®¡ç†åŒä¹‰è¯
            'è´¨é‡ç®¡ç†': 'ISO9001',
            'å“è´¨æ£€éªŒ': 'è´¨é‡ç®¡ç†',
            'IQC': 'è´¨é‡ç®¡ç†',
            'PQC': 'è´¨é‡ç®¡ç†',
            'å…­è¥¿æ ¼ç›': '6sigma',
            
            # ç‰©æµç›¸å…³åŒä¹‰è¯
            'ä»“å‚¨ç®¡ç†': 'ä»“åº“ç®¡ç†',
            'åº“å­˜ç®¡ç†': 'ä»“åº“ç®¡ç†',
            'ç‰©æµé…é€': 'ç‰©æµç®¡ç†',
            'ä¾›åº”é“¾ç®¡ç†': 'ç‰©æµç®¡ç†',
            
            # è®¾è®¡ç›¸å…³åŒä¹‰è¯
            'ç¾å·¥': 'å¹³é¢è®¾è®¡',
            'å›¾ç‰‡å¤„ç†': 'å¹³é¢è®¾è®¡',
            'UIè®¾è®¡': 'ç•Œé¢è®¾è®¡',
            'GUIè®¾è®¡': 'ç•Œé¢è®¾è®¡',
            'è§†è§‰è®¾è®¡': 'å¹³é¢è®¾è®¡',
            
            # è´¢åŠ¡ç›¸å…³åŒä¹‰è¯
            'ä¼šè®¡æ ¸ç®—': 'è´¢åŠ¡ç®¡ç†',
            'æˆæœ¬æ ¸ç®—': 'æˆæœ¬æ§åˆ¶',
            'è´¢åŠ¡æŠ¥è¡¨ç¼–åˆ¶': 'è´¢åŠ¡åˆ†æ',
            'ç¨åŠ¡ç”³æŠ¥': 'ç¨åŠ¡ç®¡ç†',
            'èµ„é‡‘ç®¡ç†': 'è´¢åŠ¡ç®¡ç†',
            
            # ç”Ÿäº§ç›¸å…³åŒä¹‰è¯
            'ç”Ÿäº§è®¡åˆ’': 'ç”Ÿäº§ç®¡ç†',
            'ç”Ÿäº§è°ƒåº¦': 'ç”Ÿäº§ç®¡ç†',
            'ç°åœºç®¡ç†': 'ç”Ÿäº§ç®¡ç†',
            'è®¾å¤‡ç®¡ç†': 'è®¾å¤‡ç»´æŠ¤',
            'å·¥è‰ºå¼€å‘': 'å·¥è‰ºç®¡ç†',
            
            # äººåŠ›èµ„æºåŒä¹‰è¯
            'æ‹›è˜ç®¡ç†': 'äººåŠ›èµ„æºç®¡ç†',
            'è–ªé…¬ç®¡ç†': 'äººåŠ›èµ„æºç®¡ç†',
            'ç»©æ•ˆè€ƒæ ¸': 'äººåŠ›èµ„æºç®¡ç†',
            'åŸ¹è®­ç®¡ç†': 'åŸ¹è®­ä¸å‘å±•',
            'å‘˜å·¥å…³ç³»ç®¡ç†': 'äººåŠ›èµ„æºç®¡ç†'
        }
    
    def _load_stop_words(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯å’Œå™ªå£°è¯"""
        return {
            # æ•°å­—å’Œç¬¦å·
            '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',
            'å¹´', 'æœˆ', 'æ—¥', 'æ¬¡', 'ä¸ª', 'ä½', 'å', 'äºº',
            'com', 'cn', 'www', 'http', 'https',
            
            # æ— æ„ä¹‰çš„å½¢å®¹è¯
            'è‰¯å¥½', 'è¾ƒå¼º', 'ä¼˜ç§€', 'å‡ºè‰²', 'å…·å¤‡', 'å…·æœ‰', 'æ‹¥æœ‰',
            'èƒ½å¤Ÿ', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'è¦æ±‚', 'å¸Œæœ›',
            
            # å¸¸è§çš„è¿æ¥è¯
            'ä»¥ä¸Š', 'ä»¥ä¸‹', 'æˆ–è€…', 'ä»¥åŠ', 'å¹¶ä¸”', 'åŒæ—¶',
            'ç›¸å…³', 'ç›¸åº”', 'å¯¹åº”', 'åˆé€‚', 'é€‚åˆ',
            
            # èŒä½ç›¸å…³ä½†éæŠ€èƒ½çš„è¯
            'å·¥ä½œ', 'å²—ä½', 'èŒä½', 'è´Ÿè´£', 'æ‰¿æ‹…', 'å®Œæˆ',
            'å®æ–½', 'æ‰§è¡Œ', 'æ¨è¿›', 'è½å®', 'å¼€å±•'
        }
    
    def _compile_skill_patterns(self) -> Dict[str, re.Pattern]:
        """ç¼–è¯‘æŠ€èƒ½ç›¸å…³æ­£åˆ™è¡¨è¾¾å¼"""
        return {
            'education': re.compile(r'(æœ¬ç§‘|ç¡•å£«|ç ”ç©¶ç”Ÿ|åšå£«|å¤§ä¸“|ä¸“ç§‘|MBA|EMBA)(?:ä»¥ä¸Š|åŠä»¥ä¸Š)?'),
            'experience_years': re.compile(r'(\d+)å¹´ä»¥ä¸Š.*?ç»éªŒ'),
            'experience_field': re.compile(r'(\d+)å¹´ä»¥ä¸Š.*?(ç®¡ç†|å¼€å‘|é”€å”®|è®¾è®¡|è¿è¥|å¸‚åœº|æŠ€æœ¯)ç»éªŒ'),
            'skill_proficiency': re.compile(r'(ç†Ÿç»ƒæŒæ¡|ç²¾é€š|ç†Ÿæ‚‰|äº†è§£)\s*([^\sï¼Œã€‚ï¼›]{2,10})'),
            'ability_requirement': re.compile(r'(å…·å¤‡|å…·æœ‰|æ‹¥æœ‰).*?(æ²Ÿé€š|åè°ƒ|ç®¡ç†|åˆ†æ|å­¦ä¹ |åˆ›æ–°|é¢†å¯¼)èƒ½åŠ›'),
            'good_ability': re.compile(r'(è‰¯å¥½|è¾ƒå¼º|ä¼˜ç§€|å‡ºè‰²)çš„\s*(æ²Ÿé€š|å›¢é˜Ÿ|åˆ†æ|å­¦ä¹ |ç®¡ç†|åè°ƒ|ç»„ç»‡)èƒ½åŠ›')
        }
    
    def _add_custom_dict(self):
        """æ·»åŠ è‡ªå®šä¹‰è¯å…¸"""
        all_skills = []
        for skills in self.skill_keywords.values():
            all_skills.extend(skills)
        
        for skill in all_skills:
            jieba.add_word(skill, freq=1000, tag='skill')
        
        # æ·»åŠ åŒä¹‰è¯
        for synonym in self.synonyms_dict.keys():
            jieba.add_word(synonym, freq=800, tag='skill')
    
    def clean_text(self, text: str) -> str:
        """æ·±åº¦æ¸…ç†æ–‡æœ¬"""
        if pd.isna(text) or text == '':
            return ''
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤é‚®ç®±å’Œç½‘å€
        text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
        text = re.sub(r'http[s]?://[^\s]+', '', text)
        text = re.sub(r'www\.[^\s]+', '', text)
        
        # ç§»é™¤ç”µè¯å·ç 
        text = re.sub(r'\d{3,4}-?\d{7,8}', '', text)
        text = re.sub(r'1[3-9]\d{9}', '', text)
        
        # æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')
        text = text.replace('ï¼›', ';').replace('ï¼š', ':')
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\+\-\.\s,ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿã€ï¼ˆï¼‰\[\]{}()]', ' ', text)
        
        # ç»Ÿä¸€ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def filter_skill(self, skill: str) -> bool:
        """è¿‡æ»¤æŠ€èƒ½ï¼Œå»é™¤å™ªå£°"""
        if not skill or len(skill.strip()) < 2:
            return False
        
        skill = skill.strip()
        
        # è¿‡æ»¤åœç”¨è¯
        if skill in self.stop_words:
            return False
        
        # è¿‡æ»¤çº¯æ•°å­—
        if skill.isdigit():
            return False
        
        # è¿‡æ»¤åŒ…å«è¿‡å¤šæ•°å­—çš„è¯ï¼ˆå¦‚"1ç®¡ç†"ã€"3å¹´ç»éªŒ"ï¼‰
        if re.match(r'^\d+\s*\w*$', skill):
            return False
        
        # è¿‡æ»¤å•ä¸ªå­—ç¬¦æˆ–å¤ªé•¿çš„è¯
        if len(skill) < 2 or len(skill) > 15:
            return False
        
        # è¿‡æ»¤åªæœ‰æ ‡ç‚¹ç¬¦å·çš„
        if re.match(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+$', skill):
            return False
        
        return True
    
    def extract_by_keywords(self, text: str) -> Dict[str, List[str]]:
        """åŸºäºå…³é”®è¯æå–æŠ€èƒ½"""
        text_lower = text.lower()
        extracted = defaultdict(list)
        
        for category, keywords in self.skill_keywords.items():
            for keyword in keywords:
                # ç²¾ç¡®åŒ¹é…
                if keyword.lower() in text_lower:
                    extracted[category].append(keyword)
        
        # æ¸…ç†ç»“æœ
        cleaned_extracted = {}
        for category, skills in extracted.items():
            cleaned_skills = [skill for skill in set(skills) if self.filter_skill(skill)]
            if cleaned_skills:
                cleaned_extracted[category] = cleaned_skills
        
        return cleaned_extracted
    
    def extract_by_patterns(self, text: str) -> Dict[str, List[str]]:
        """åŸºäºæ­£åˆ™è¡¨è¾¾å¼æå–"""
        extracted = defaultdict(list)
        
        for pattern_name, pattern in self.skill_patterns.items():
            matches = pattern.findall(text)
            for match in matches:
                if isinstance(match, tuple):
                    # å¤„ç†åˆ†ç»„åŒ¹é…
                    if pattern_name == 'skill_proficiency':
                        skill = match[1].strip()
                        if self.filter_skill(skill):
                            extracted['proficiency_skills'].append(skill)
                    elif pattern_name in ['good_ability', 'ability_requirement']:
                        ability = match[-1] + 'èƒ½åŠ›'  # æœ€åä¸€ä¸ªåŒ¹é…ç»„ + 'èƒ½åŠ›'
                        if self.filter_skill(ability):
                            extracted['extracted_abilities'].append(ability)
                    else:
                        clean_match = ' '.join(match).strip()
                        if self.filter_skill(clean_match):
                            extracted[pattern_name].append(clean_match)
                else:
                    if self.filter_skill(match):
                        extracted[pattern_name].append(match)
        
        return dict(extracted)
    
    def extract_by_segmentation(self, text: str) -> List[str]:
        """åŸºäºåˆ†è¯æå–æ½œåœ¨æŠ€èƒ½"""
        words = jieba.lcut(text)
        skills = []
        
        # æŠ€èƒ½æŒ‡ç¤ºè¯
        skill_indicators = ['èƒ½åŠ›', 'æŠ€èƒ½', 'æŠ€å·§', 'ç»éªŒ', 'ç®¡ç†', 'å¼€å‘', 'è®¾è®¡', 'åˆ†æ', 'è¿è¥']
        
        for word in words:
            # è¿‡æ»¤åŸºæœ¬æ¡ä»¶
            if not self.filter_skill(word):
                continue
            
            # åŒ…å«æŠ€èƒ½æŒ‡ç¤ºè¯çš„å¤åˆè¯
            if any(indicator in word for indicator in skill_indicators) and len(word) >= 3:
                skills.append(word)
            # è‹±æ–‡æŠ€æœ¯è¯æ±‡
            elif re.match(r'^[a-zA-Z][a-zA-Z0-9\+\-\.]*$', word) and len(word) >= 3:
                skills.append(word)
            # åœ¨å·²çŸ¥æŠ€èƒ½åˆ—è¡¨ä¸­
            elif word in [skill for skills in self.skill_keywords.values() for skill in skills]:
                skills.append(word)
        
        return list(set(skills))
    
    def normalize_skills(self, skills: List[str]) -> List[str]:
        """æ ‡å‡†åŒ–æŠ€èƒ½ï¼ˆå¤„ç†åŒä¹‰è¯ï¼‰"""
        normalized = []
        
        for skill in skills:
            # æ¸…ç†ç©ºæ ¼å’Œæ ‡ç‚¹
            skill = re.sub(r'\s+', '', skill)
            skill = skill.strip('.,;:!?()[]{}')
            
            if not self.filter_skill(skill):
                continue
            
            # æŸ¥æ‰¾åŒä¹‰è¯æ˜ å°„
            normalized_skill = self.synonyms_dict.get(skill, skill)
            normalized.append(normalized_skill)
        
        return list(set(normalized))
    
    def extract_skills_from_description(self, description: str) -> Dict:
        """ä»å²—ä½æè¿°ä¸­æå–æŠ€èƒ½"""
        if pd.isna(description) or description.strip() == '':
            return {
                'keywords_skills': {},
                'pattern_skills': {},
                'segmentation_skills': [],
                'normalized_skills': [],
                'total_skills': 0,
                'text_length': 0
            }
        
        # æ·±åº¦æ¸…ç†æ–‡æœ¬
        clean_text = self.clean_text(description)
        
        # å¤šç§æ–¹æ³•æå–
        keywords_skills = self.extract_by_keywords(clean_text)
        pattern_skills = self.extract_by_patterns(clean_text)
        segmentation_skills = self.extract_by_segmentation(clean_text)
        
        # æ”¶é›†æ‰€æœ‰æŠ€èƒ½
        all_skills = []
        for skill_list in keywords_skills.values():
            all_skills.extend(skill_list)
        for skill_list in pattern_skills.values():
            all_skills.extend(skill_list)
        all_skills.extend(segmentation_skills)
        
        # æ ‡å‡†åŒ–å¤„ç†
        normalized_skills = self.normalize_skills(all_skills)
        
        return {
            'keywords_skills': keywords_skills,
            'pattern_skills': pattern_skills,
            'segmentation_skills': segmentation_skills,
            'normalized_skills': normalized_skills,
            'total_skills': len(normalized_skills),
            'text_length': len(clean_text)
        }
    
    def process_job_descriptions(self, df: pd.DataFrame, desc_column: str = 'å²—ä½æè¿°') -> pd.DataFrame:
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†çš„å²—ä½æè¿°åˆ—"""
        print(f"å¼€å§‹å¤„ç†å²—ä½æè¿°æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•...")
        
        result_df = df.copy()
        
        # æ·»åŠ æ–°åˆ—
        result_df['æå–çš„æŠ€èƒ½'] = None
        result_df['æŠ€èƒ½æ•°é‡'] = 0
        result_df['ç¼–ç¨‹æŠ€èƒ½'] = None
        result_df['æ²Ÿé€šæŠ€èƒ½'] = None
        result_df['é¢†å¯¼æŠ€èƒ½'] = None
        result_df['åˆ†ææŠ€èƒ½'] = None
        result_df['ä¸šåŠ¡æŠ€èƒ½'] = None
        result_df['æ ‡å‡†åŒ–æŠ€èƒ½'] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_skills_found = 0
        jobs_with_skills = 0
        
        for idx, row in result_df.iterrows():
            if idx % 500 == 0:
                print(f"å·²å¤„ç† {idx}/{len(df)} æ¡è®°å½•...")
            
            # æå–æŠ€èƒ½
            skill_data = self.extract_skills_from_description(row[desc_column])
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            result_df.at[idx, 'æå–çš„æŠ€èƒ½'] = json.dumps(skill_data, ensure_ascii=False)
            result_df.at[idx, 'æŠ€èƒ½æ•°é‡'] = skill_data['total_skills']
            result_df.at[idx, 'æ ‡å‡†åŒ–æŠ€èƒ½'] = json.dumps(skill_data['normalized_skills'], ensure_ascii=False)
            
            # åˆ†ç±»ä¿å­˜
            keywords_skills = skill_data['keywords_skills']
            result_df.at[idx, 'ç¼–ç¨‹æŠ€èƒ½'] = json.dumps(keywords_skills.get('programming', []), ensure_ascii=False)
            result_df.at[idx, 'æ²Ÿé€šæŠ€èƒ½'] = json.dumps(keywords_skills.get('communication_skills', []), ensure_ascii=False)
            result_df.at[idx, 'é¢†å¯¼æŠ€èƒ½'] = json.dumps(keywords_skills.get('leadership_skills', []), ensure_ascii=False)
            result_df.at[idx, 'åˆ†ææŠ€èƒ½'] = json.dumps(keywords_skills.get('analytical_skills', []), ensure_ascii=False)
            result_df.at[idx, 'ä¸šåŠ¡æŠ€èƒ½'] = json.dumps(keywords_skills.get('business_skills', []), ensure_ascii=False)
            
            # ç»Ÿè®¡
            if skill_data['total_skills'] > 0:
                jobs_with_skills += 1
                total_skills_found += skill_data['total_skills']
        
        print(f"\nå¤„ç†å®Œæˆï¼")
        print(f"æ€»èŒä½æ•°: {len(df)}")
        print(f"åŒ…å«æŠ€èƒ½çš„èŒä½æ•°: {jobs_with_skills}")
        print(f"æŠ€èƒ½è¦†ç›–ç‡: {jobs_with_skills/len(df)*100:.1f}%")
        print(f"å¹³å‡æ¯èŒä½æŠ€èƒ½æ•°: {total_skills_found/len(df):.1f}")
        
        return result_df
    
    def generate_skill_report(self, df: pd.DataFrame) -> Dict:
        """ç”ŸæˆæŠ€èƒ½åˆ†ææŠ¥å‘Š"""
        print("ç”ŸæˆæŠ€èƒ½åˆ†ææŠ¥å‘Š...")
        
        all_skills = Counter()
        skill_categories = Counter()
        
        for idx, row in df.iterrows():
            # ç»Ÿè®¡æ ‡å‡†åŒ–æŠ€èƒ½
            normalized_skills_str = row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]')
            try:
                normalized_skills = json.loads(normalized_skills_str)
                for skill in normalized_skills:
                    all_skills[skill] += 1
            except:
                continue
            
            # ç»Ÿè®¡åˆ†ç±»æŠ€èƒ½
            skill_data_str = row.get('æå–çš„æŠ€èƒ½', '{}')
            try:
                skill_data = json.loads(skill_data_str)
                keywords_skills = skill_data.get('keywords_skills', {})
                for category, skills in keywords_skills.items():
                    skill_categories[category] += len(skills)
            except:
                continue
        
        # è½¬æ¢pandasæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        skill_stats = df['æŠ€èƒ½æ•°é‡'].describe()
        skill_distribution = {
            'count': int(skill_stats['count']),
            'mean': float(skill_stats['mean']),
            'std': float(skill_stats['std']),
            'min': int(skill_stats['min']),
            '25%': float(skill_stats['25%']),
            '50%': float(skill_stats['50%']),
            '75%': float(skill_stats['75%']),
            'max': int(skill_stats['max'])
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'summary': {
                'total_jobs': int(len(df)),
                'jobs_with_skills': int(len(df[df['æŠ€èƒ½æ•°é‡'] > 0])),
                'total_unique_skills': int(len(all_skills)),
                'avg_skills_per_job': float(df['æŠ€èƒ½æ•°é‡'].mean()),
                'max_skills_per_job': int(df['æŠ€èƒ½æ•°é‡'].max())
            },
            'top_skills': [(skill, int(count)) for skill, count in all_skills.most_common(50)],
            'skill_categories': {k: int(v) for k, v in skill_categories.items()},
            'skill_distribution': skill_distribution
        }
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–æ•°æ®
    print("è¯»å–æ•°æ®...")
    try:
        df = pd.read_csv('lunwen/newjob1_sortall.csv')
        print(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
    except Exception as e:
        print(f"è¯»å–æ•°æ®å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥å²—ä½æè¿°åˆ—
    if 'å²—ä½æè¿°' not in df.columns:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°'å²—ä½æè¿°'åˆ—")
        print(f"å¯ç”¨åˆ—: {list(df.columns)}")
        return
    
    # åˆå§‹åŒ–æ”¹è¿›ç‰ˆæå–å™¨
    extractor = ImprovedSkillExtractor()
    
    # ===== ä¿®æ”¹è¿™é‡Œæ¥å¤„ç†å…¨éƒ¨æ•°æ® =====
    # é€‰æ‹©è¦å¤„ç†çš„æ•°æ®é‡
    process_all_data = True  # è®¾ç½®ä¸ºTrueå¤„ç†å…¨éƒ¨æ•°æ®ï¼ŒFalseå¤„ç†æ ·æœ¬
    
    if process_all_data:
        # å¤„ç†å…¨éƒ¨æ•°æ®
        sample_df = df.copy()
        print(f"å°†å¤„ç†å…¨éƒ¨ {len(sample_df)} æ¡è®°å½•")
        output_file = 'all_jobs_skills_extracted.csv'  # å…¨é‡æ•°æ®è¾“å‡ºæ–‡ä»¶å
        report_file = 'all_jobs_skill_analysis_report.json'
        isco_file = 'all_jobs_isco_skill_distribution.json'
    else:
        # å¤„ç†æ ·æœ¬æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        sample_size = min(5000, len(df))
        sample_df = df.head(sample_size).copy()
        print(f"å°†å¤„ç†å‰ {len(sample_df)} æ¡è®°å½•ï¼ˆæ ·æœ¬æ¨¡å¼ï¼‰")
        output_file = 'sample_jobs_skills_extracted.csv'
        report_file = 'sample_skill_analysis_report.json'
        isco_file = 'sample_isco_skill_distribution.json'
    
    # å¤„ç†æ•°æ®
    print("å¼€å§‹æŠ€èƒ½æå–...")
    start_time = pd.Timestamp.now()
    
    result_df = extractor.process_job_descriptions(sample_df)
    
    end_time = pd.Timestamp.now()
    processing_time = (end_time - start_time).total_seconds()
    print(f"å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {processing_time:.2f} ç§’")
    print(f"å¹³å‡æ¯æ¡è®°å½•å¤„ç†æ—¶é—´: {processing_time/len(sample_df):.3f} ç§’")
    
    # ä¿å­˜ç»“æœ
    print(f"ä¿å­˜ç»“æœåˆ°: {output_file}")
    try:
        result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"âœ“ CSVæ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        import os
        file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
        print(f"âœ“ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
    except Exception as e:
        print(f"âœ— CSVæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    report = extractor.generate_skill_report(result_df)
    
    print("\n" + "="*60)
    print("æ”¹è¿›ç‰ˆæŠ€èƒ½æå–æŠ¥å‘Š")
    print("="*60)
    
    summary = report['summary']
    print(f"æ€»èŒä½æ•°: {summary['total_jobs']:,}")
    print(f"åŒ…å«æŠ€èƒ½çš„èŒä½æ•°: {summary['jobs_with_skills']:,}")
    print(f"æŠ€èƒ½è¦†ç›–ç‡: {summary['jobs_with_skills']/summary['total_jobs']*100:.1f}%")
    print(f"è¯†åˆ«çš„å”¯ä¸€æŠ€èƒ½æ•°: {summary['total_unique_skills']:,}")
    print(f"å¹³å‡æ¯èŒä½æŠ€èƒ½æ•°: {summary['avg_skills_per_job']:.1f}")
    print(f"æœ€å¤šæŠ€èƒ½æ•°: {summary['max_skills_per_job']:,}")
    
    print(f"\nå‰30ä¸ªæœ€å¸¸è§æŠ€èƒ½:")
    for i, (skill, count) in enumerate(report['top_skills'][:30], 1):
        print(f"  {i:2d}. {skill}: {count:,}")
    
    print(f"\næŠ€èƒ½åˆ†ç±»ç»Ÿè®¡:")
    for category, count in sorted(report['skill_categories'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count:,}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    except Exception as e:
        print(f"\nâœ— æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ç»“æœ
    print(f"\nç¤ºä¾‹æå–ç»“æœ:")
    for i in range(min(5, len(result_df))):
        row = result_df.iloc[i]
        job_title = row.get('å²—ä½', 'N/A')
        skills_count = row.get('æŠ€èƒ½æ•°é‡', 0)
        try:
            normalized_skills = json.loads(row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]'))
            skills_display = normalized_skills[:8]  # æ˜¾ç¤ºå‰8ä¸ªæŠ€èƒ½
        except:
            skills_display = []
        
        print(f"\n{i+1}. ã€{job_title}ã€‘- æŠ€èƒ½æ•°é‡: {skills_count}")
        print(f"   ä¸»è¦æŠ€èƒ½: {skills_display}")
        
        # æ˜¾ç¤ºåˆ†ç±»æŠ€èƒ½
        try:
            programming_skills = json.loads(row.get('ç¼–ç¨‹æŠ€èƒ½', '[]'))
            communication_skills = json.loads(row.get('æ²Ÿé€šæŠ€èƒ½', '[]'))
            business_skills = json.loads(row.get('ä¸šåŠ¡æŠ€èƒ½', '[]'))
            
            if programming_skills:
                print(f"   ç¼–ç¨‹æŠ€èƒ½: {programming_skills}")
            if communication_skills:
                print(f"   æ²Ÿé€šæŠ€èƒ½: {communication_skills}")
            if business_skills:
                print(f"   ä¸šåŠ¡æŠ€èƒ½: {business_skills[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª
        except:
            pass
    
    # æŒ‰ISCOä»£ç åˆ†ææŠ€èƒ½åˆ†å¸ƒ
    if 'ISCO_4_Digit_Code_Gemini' in result_df.columns:
        print(f"\n" + "="*60)
        print("æŒ‰ISCOèŒä¸šä»£ç åˆ†ææŠ€èƒ½åˆ†å¸ƒ")
        print("="*60)
        
        isco_skill_analysis = analyze_skills_by_isco(result_df)
        
        # æŒ‰èŒä½æ•°é‡æ’åºï¼Œæ˜¾ç¤ºæ›´å¤šISCOç±»åˆ«
        isco_sorted = sorted(isco_skill_analysis.items(), 
                           key=lambda x: x[1]['job_count'], reverse=True)
        
        # æ˜¾ç¤ºå‰20ä¸ªISCOç±»åˆ«ï¼ˆæˆ–å…¨éƒ¨ï¼Œå¦‚æœå°‘äº20ä¸ªï¼‰
        display_count = min(20, len(isco_sorted))
        print(f"æ˜¾ç¤ºå‰{display_count}ä¸ªèŒä¸šç±»åˆ«çš„æŠ€èƒ½åˆ†å¸ƒï¼š\n")
        
        for i, (isco_code, analysis) in enumerate(isco_sorted[:display_count]):
            print(f"{i+1:2d}. ISCO {isco_code} (å…±{analysis['job_count']:,}ä¸ªèŒä½):")
            print(f"    å¹³å‡æŠ€èƒ½æ•°: {analysis['avg_skills']:.1f}")
            print(f"    æŠ€èƒ½å¤šæ ·æ€§: {analysis['skill_diversity']}ç§ä¸åŒæŠ€èƒ½")
            
            # æ˜¾ç¤ºå‰8ä¸ªå¸¸è§æŠ€èƒ½ï¼Œæ ¼å¼åŒ–è¾“å‡º
            top_skills = analysis['top_skills'][:8]
            if top_skills:
                skills_str = ", ".join([f"{skill}({count})" for skill, count in top_skills])
                print(f"    ä¸»è¦æŠ€èƒ½: {skills_str}")
            else:
                print(f"    ä¸»è¦æŠ€èƒ½: æš‚æ— ")
            print()  # ç©ºè¡Œåˆ†éš”
        
        # æ·»åŠ æŠ€èƒ½åˆ†å¸ƒç»Ÿè®¡
        print("="*60)
        print("ISCOèŒä¸šæŠ€èƒ½åˆ†å¸ƒç»Ÿè®¡")
        print("="*60)
        
        # è®¡ç®—å„ç§ç»Ÿè®¡æŒ‡æ ‡
        job_counts = [analysis['job_count'] for analysis in isco_skill_analysis.values()]
        avg_skills = [analysis['avg_skills'] for analysis in isco_skill_analysis.values()]
        skill_diversity = [analysis['skill_diversity'] for analysis in isco_skill_analysis.values()]
        
        print(f"æ€»èŒä¸šç±»åˆ«æ•°: {len(isco_skill_analysis)}")
        print(f"èŒä½æ•°é‡åˆ†å¸ƒ: æœ€å¤š{max(job_counts):,}ä¸ª, æœ€å°‘{min(job_counts)}ä¸ª, å¹³å‡{sum(job_counts)/len(job_counts):.1f}ä¸ª")
        print(f"å¹³å‡æŠ€èƒ½æ•°åˆ†å¸ƒ: æœ€é«˜{max(avg_skills):.1f}, æœ€ä½{min(avg_skills):.1f}, æ€»ä½“å¹³å‡{sum(avg_skills)/len(avg_skills):.1f}")
        print(f"æŠ€èƒ½å¤šæ ·æ€§åˆ†å¸ƒ: æœ€é«˜{max(skill_diversity)}, æœ€ä½{min(skill_diversity)}, å¹³å‡{sum(skill_diversity)/len(skill_diversity):.1f}")
        
        # æ‰¾å‡ºæŠ€èƒ½è¦æ±‚æœ€é«˜å’Œæœ€ä½çš„èŒä¸š
        max_skill_isco = max(isco_skill_analysis.items(), key=lambda x: x[1]['avg_skills'])
        min_skill_isco = min(isco_skill_analysis.items(), key=lambda x: x[1]['avg_skills'])
        
        print(f"\næŠ€èƒ½è¦æ±‚æœ€é«˜çš„èŒä¸š: ISCO {max_skill_isco[0]} (å¹³å‡{max_skill_isco[1]['avg_skills']:.1f}ä¸ªæŠ€èƒ½)")
        print(f"æŠ€èƒ½è¦æ±‚æœ€ä½çš„èŒä¸š: ISCO {min_skill_isco[0]} (å¹³å‡{min_skill_isco[1]['avg_skills']:.1f}ä¸ªæŠ€èƒ½)")
        
        # æ‰¾å‡ºæŠ€èƒ½æœ€å¤šæ ·åŒ–çš„èŒä¸š
        max_diversity_isco = max(isco_skill_analysis.items(), key=lambda x: x[1]['skill_diversity'])
        print(f"æŠ€èƒ½æœ€å¤šæ ·åŒ–çš„èŒä¸š: ISCO {max_diversity_isco[0]} ({max_diversity_isco[1]['skill_diversity']}ç§ä¸åŒæŠ€èƒ½)")
        
        # å±•ç¤ºä¸åŒç±»åˆ«çš„ä»£è¡¨æ€§èŒä¸š
        print(f"\n" + "="*60)
        print("ä¸åŒæŠ€èƒ½è¦æ±‚æ°´å¹³çš„ä»£è¡¨èŒä¸š")
        print("="*60)
        
        # å°†èŒä¸šæŒ‰å¹³å‡æŠ€èƒ½æ•°åˆ†ç»„
        high_skill_jobs = [item for item in isco_sorted if item[1]['avg_skills'] >= 10]
        medium_skill_jobs = [item for item in isco_sorted if 5 <= item[1]['avg_skills'] < 10]
        low_skill_jobs = [item for item in isco_sorted if item[1]['avg_skills'] < 5]
        
        if high_skill_jobs:
            print(f"é«˜æŠ€èƒ½è¦æ±‚èŒä¸š (â‰¥10ä¸ªæŠ€èƒ½, å…±{len(high_skill_jobs)}ç±»):")
            for isco_code, analysis in high_skill_jobs[:5]:
                top_3_skills = [skill for skill, count in analysis['top_skills'][:3]]
                print(f"  ISCO {isco_code}: {analysis['avg_skills']:.1f}ä¸ªæŠ€èƒ½ - {', '.join(top_3_skills)}")
        
        if medium_skill_jobs:
            print(f"\nä¸­ç­‰æŠ€èƒ½è¦æ±‚èŒä¸š (5-9ä¸ªæŠ€èƒ½, å…±{len(medium_skill_jobs)}ç±»):")
            for isco_code, analysis in medium_skill_jobs[:5]:
                top_3_skills = [skill for skill, count in analysis['top_skills'][:3]]
                print(f"  ISCO {isco_code}: {analysis['avg_skills']:.1f}ä¸ªæŠ€èƒ½ - {', '.join(top_3_skills)}")
        
        if low_skill_jobs:
            print(f"\nåŸºç¡€æŠ€èƒ½è¦æ±‚èŒä¸š (<5ä¸ªæŠ€èƒ½, å…±{len(low_skill_jobs)}ç±»):")
            for isco_code, analysis in low_skill_jobs[:5]:
                top_3_skills = [skill for skill, count in analysis['top_skills'][:3]]
                print(f"  ISCO {isco_code}: {analysis['avg_skills']:.1f}ä¸ªæŠ€èƒ½ - {', '.join(top_3_skills)}")
        
        # ä¿å­˜ISCOåˆ†æç»“æœ
        try:
            with open(isco_file, 'w', encoding='utf-8') as f:
                json.dump(isco_skill_analysis, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ ISCOæŠ€èƒ½åˆ†æå·²ä¿å­˜åˆ°: {isco_file}")
        except Exception as e:
            print(f"\nâœ— ISCOåˆ†æä¿å­˜å¤±è´¥: {e}")
    
    # æä¾›æ•°æ®ä½¿ç”¨å»ºè®®
    print(f"\n" + "="*60)
    print("æ•°æ®ä½¿ç”¨å»ºè®®")
    print("="*60)
    print("ğŸ“ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
    print(f"  â€¢ {output_file} - åŒ…å«æŠ€èƒ½çš„å®Œæ•´æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰")
    print(f"  â€¢ {report_file} - è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰")
    print(f"  â€¢ {isco_file} - ISCOæŠ€èƒ½åˆ†æï¼ˆJSONæ ¼å¼ï¼‰")
    
    print("\nğŸ“Š åç»­åˆ†æå»ºè®®:")
    print("  â€¢ å¯ç›´æ¥ç”¨Excel/Python/Råˆ†æCSVæ–‡ä»¶")
    print("  â€¢ 'æ ‡å‡†åŒ–æŠ€èƒ½'åˆ—é€‚åˆåšçŸ¥è¯†å›¾è°±èŠ‚ç‚¹")
    print("  â€¢ å„åˆ†ç±»æŠ€èƒ½åˆ—ä¾¿äºè¡Œä¸šåˆ†æ")
    print("  â€¢ ISCOåˆ†æå¯ç”¨äºèŒä¸šæŠ€èƒ½ç”»åƒ")
    
    print("\nğŸ”— çŸ¥è¯†å›¾è°±æ„å»º:")
    print("  â€¢ èŠ‚ç‚¹: ISCOèŒä¸šä»£ç  + æ ‡å‡†åŒ–æŠ€èƒ½")
    print("  â€¢ è¾¹æƒé‡: æŠ€èƒ½åœ¨èŒä¸šä¸­çš„å‡ºç°é¢‘ç‡")
    print("  â€¢ å¯è¿›ä¸€æ­¥è®¡ç®—æŠ€èƒ½ç›¸ä¼¼åº¦å’ŒèŒä¸šè½¬æ¢è·¯å¾„")

# æ·»åŠ æ‰¹é‡å¤„ç†çš„è¾…åŠ©å‡½æ•°
def process_in_batches(df: pd.DataFrame, extractor, batch_size: int = 1000) -> pd.DataFrame:
    """åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†ï¼Œé¿å…å†…å­˜é—®é¢˜"""
    total_batches = (len(df) + batch_size - 1) // batch_size
    print(f"å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡è®°å½•")
    
    processed_dfs = []
    
    for i in range(0, len(df), batch_size):
        batch_num = i // batch_size + 1
        batch_df = df.iloc[i:i+batch_size].copy()
        
        print(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_df)} æ¡è®°å½•)...")
        
        batch_result = extractor.process_job_descriptions(batch_df)
        processed_dfs.append(batch_result)
        
        # å¯é€‰ï¼šæ¯æ‰¹å¤„ç†å®Œåé‡Šæ”¾å†…å­˜
        import gc
        gc.collect()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    print("åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ...")
    final_result = pd.concat(processed_dfs, ignore_index=True)
    
    return final_result
    with open('improved_skill_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: improved_skill_analysis_report.json")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ç»“æœ
    print(f"\nç¤ºä¾‹æå–ç»“æœ:")
    for i in range(min(5, len(result_df))):
        row = result_df.iloc[i]
        job_title = row.get('å²—ä½', 'N/A')
        skills_count = row.get('æŠ€èƒ½æ•°é‡', 0)
        try:
            normalized_skills = json.loads(row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]'))
            skills_display = normalized_skills[:8]  # æ˜¾ç¤ºå‰8ä¸ªæŠ€èƒ½
        except:
            skills_display = []
        
        print(f"\n{i+1}. ã€{job_title}ã€‘- æŠ€èƒ½æ•°é‡: {skills_count}")
        print(f"   ä¸»è¦æŠ€èƒ½: {skills_display}")
        
        # æ˜¾ç¤ºåˆ†ç±»æŠ€èƒ½
        try:
            programming_skills = json.loads(row.get('ç¼–ç¨‹æŠ€èƒ½', '[]'))
            communication_skills = json.loads(row.get('æ²Ÿé€šæŠ€èƒ½', '[]'))
            business_skills = json.loads(row.get('ä¸šåŠ¡æŠ€èƒ½', '[]'))
            
            if programming_skills:
                print(f"   ç¼–ç¨‹æŠ€èƒ½: {programming_skills}")
            if communication_skills:
                print(f"   æ²Ÿé€šæŠ€èƒ½: {communication_skills}")
            if business_skills:
                print(f"   ä¸šåŠ¡æŠ€èƒ½: {business_skills[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª
        except:
            pass
    
    # æŒ‰ISCOä»£ç åˆ†ææŠ€èƒ½åˆ†å¸ƒ
    if 'ISCO_4_Digit_Code_Gemini' in result_df.columns:
        print(f"\n" + "="*60)
        print("æŒ‰ISCOèŒä¸šä»£ç åˆ†ææŠ€èƒ½åˆ†å¸ƒ")
        print("="*60)
        
        isco_skill_analysis = analyze_skills_by_isco(result_df)
        
        # æ˜¾ç¤ºå‰5ä¸ªISCOç±»åˆ«çš„æŠ€èƒ½åˆ†å¸ƒ
        for i, (isco_code, analysis) in enumerate(list(isco_skill_analysis.items())[:5]):
            print(f"\nISCO {isco_code} (å…±{analysis['job_count']}ä¸ªèŒä½):")
            print(f"  å¹³å‡æŠ€èƒ½æ•°: {analysis['avg_skills']:.1f}")
            print(f"  å‰5ä¸ªå¸¸è§æŠ€èƒ½: {analysis['top_skills'][:5]}")
        
        # ä¿å­˜ISCOåˆ†æç»“æœ
        with open('isco_skill_distribution.json', 'w', encoding='utf-8') as f:
            json.dump(isco_skill_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nISCOæŠ€èƒ½åˆ†æå·²ä¿å­˜åˆ°: isco_skill_distribution.json")

def analyze_skills_by_isco(df: pd.DataFrame) -> Dict:
    """æŒ‰ISCOä»£ç åˆ†ææŠ€èƒ½åˆ†å¸ƒ"""
    isco_analysis = defaultdict(lambda: {
        'job_count': 0,
        'total_skills': 0,
        'skill_counter': Counter(),
        'avg_skills': 0.0,
        'top_skills': []
    })
    
    for idx, row in df.iterrows():
        isco_code = row.get('ISCO_4_Digit_Code_Gemini')
        if pd.isna(isco_code):
            continue
        
        isco_code = str(int(isco_code)) if isinstance(isco_code, float) else str(isco_code)
        
        # ç»Ÿè®¡èŒä½æ•°é‡
        isco_analysis[isco_code]['job_count'] += 1
        
        # ç»Ÿè®¡æŠ€èƒ½
        skills_count = row.get('æŠ€èƒ½æ•°é‡', 0)
        isco_analysis[isco_code]['total_skills'] += skills_count
        
        # ç»Ÿè®¡å…·ä½“æŠ€èƒ½
        try:
            normalized_skills = json.loads(row.get('æ ‡å‡†åŒ–æŠ€èƒ½', '[]'))
            for skill in normalized_skills:
                isco_analysis[isco_code]['skill_counter'][skill] += 1
        except:
            pass
    
    # è®¡ç®—å¹³å‡å€¼å’Œæ’åº
    final_analysis = {}
    for isco_code, data in isco_analysis.items():
        if data['job_count'] > 0:
            avg_skills = data['total_skills'] / data['job_count']
            top_skills = [(skill, count) for skill, count in data['skill_counter'].most_common(20)]
            
            final_analysis[isco_code] = {
                'job_count': data['job_count'],
                'total_skills': data['total_skills'],
                'avg_skills': round(avg_skills, 2),
                'top_skills': top_skills,
                'skill_diversity': len(data['skill_counter'])
            }
    
    return final_analysis

if __name__ == "__main__":
    main()